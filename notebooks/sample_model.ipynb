{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262383d1",
   "metadata": {},
   "source": [
    "# Testing Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178b05d",
   "metadata": {},
   "source": [
    "## VERSIONS\n",
    "\n",
    "- 01: \n",
    "    - Initial file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38313aff",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "10f3e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from importlib.metadata import version\n",
    "from importlib.metadata import PackageNotFoundError  # For handling package version errors\n",
    "from logging import Logger  # For type hinting\n",
    "from typing import List, Optional  # For type hinting\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "import torch\n",
    "from torch.nn import Module  # For type hinting\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.utils import resample\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22b46b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.3.2\n",
      "seaborn version: 0.13.2\n",
      "matplotlib version: 3.10.6\n",
      "torch version: 2.5.1\n",
      "joblib version: 1.5.2\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "package_list = [\"pandas\", \"seaborn\", \"matplotlib\", \"torch\", \"joblib\", \"tqdm\"]\n",
    "for package in package_list:\n",
    "    try:\n",
    "        print(f\"{package} version: {version(package)}\")  # Raises PackageNotFoundError if not found\n",
    "    except PackageNotFoundError:\n",
    "        print(f\"âŒ Package '{package}' not found. Please install it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b3792",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12e7bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbefac4",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de220668",
   "metadata": {},
   "source": [
    "### Creating Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fdbbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Dataset class For the Custom Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file: str = \"../Data/DataSplits/test.csv\", label_column: str = \"Label\"):\n",
    "        \"\"\"Initializer for the Dataset class.\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file containing the dataset.\n",
    "            label_column (str): The name of the column indicating the label.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)  # Assign a pandas data frame\n",
    "        except FileNotFoundError:  # Raise an error if the file is not found\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Define feature and label columns\n",
    "        self.label_column = label_column\n",
    "        # Omit the label column to create the list of feature columns\n",
    "        self.feature_columns = self.data.columns.drop([self.label_column])\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns a tuple (features, label) for the given index.\n",
    "        Args:\n",
    "            index (int): Index of the data sample to retrieve.\n",
    "        Returns:\n",
    "            tuple: (features, label) where features is a tensor of input features and label is the corresponding label.\n",
    "        \"\"\"\n",
    "        # Use 'iloc' instead of 'loc' for efficiency\n",
    "        features = self.data.iloc[index][self.feature_columns].values\n",
    "        label = self.data.iloc[index][self.label_column]  # Extract the label for the given index\n",
    "        return (torch.tensor(features, dtype=torch.float), torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the amount of samples in the dataset.\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10259682",
   "metadata": {},
   "source": [
    "### Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a335b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame, logger: Logger) -> pd.DataFrame:\n",
    "    \"\"\"Cleans the input DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to be cleaned.\n",
    "        logger (Logger): Logger object for logging information.\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    show_dataframe_info = True  # Set to True to log DataFrame info\n",
    "\n",
    "    # Log the initial state of the DataFrame\n",
    "    if show_dataframe_info:\n",
    "        logger.info(f\"Initial DataFrame shape: {df.shape}\")\n",
    "        buffer = io.StringIO()  # Create a buffer to capture the info output\n",
    "        df.info(buf=buffer)  # Store the output into the buffer\n",
    "        logger.info(f\"Initial DataFrame info:\\n \" + buffer.getvalue())\n",
    "\n",
    "    # Example cleaning steps (customize as needed)\n",
    "    df = df.drop_duplicates()  # Remove duplicates\n",
    "    df = df.dropna()  # Drop rows with missing values\n",
    "\n",
    "    if show_dataframe_info:\n",
    "        # Reinitialize the buffer to clear any previous content in order to log the final dataframe info\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        logger.info(f\"Final DataFrame info:\\n \" + buffer.getvalue())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9b5c1",
   "metadata": {},
   "source": [
    "### Data Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9335a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(\n",
    "    logger: Logger,\n",
    "    dataset_url: str,\n",
    "    root_data_dir: str = \"../Data\",\n",
    "    data_file_path: str = \"Dataset.csv\",\n",
    "    data_splits_dir: str = \"DataSplits\",\n",
    "    scaler_dir=\"Scalers\",\n",
    "    target_column: str = \"Target\",\n",
    "    extra_dropped_columns: Optional[List[str]] = None,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = False,\n",
    "    drop_last: bool = True,\n",
    ") -> tuple[\n",
    "    Dataset, Dataset, Dataset, DataLoader, DataLoader, DataLoader, MinMaxScaler, MinMaxScaler\n",
    "]:\n",
    "    \"\"\"This function prepares the train, test, and validation datasets.\n",
    "    Args:\n",
    "        logger (Logger): The logger instance to log messages.\n",
    "        dataset_url (str): The URL to download the dataset from, if not found locally.\n",
    "        root_data_dir (str): The root of the Data Directory\n",
    "        data_file_path (str): The name of the original dataset (with .csv file extension).\n",
    "        data_splits_dir (str): Path to the train, test, and validation datasets.\n",
    "        scaler_dir (str): Path to the feature and label scalers.\n",
    "        target_column (str): The name of the target column to predict.\n",
    "        extra_dropped_columns (List[str], optional): Additional columns to drop from the features.\n",
    "        batch_size (int): The dataloader's batch_size.\n",
    "        num_workers (int): The dataloader's number of workers.\n",
    "        pin_memory (bool): The dataloader's pin memory option.\n",
    "        drop_last (bool): The dataloader's drop_last option.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (Dataset): Dataset Class for the training dataset.\n",
    "        test_dataset (Dataset): Dataset Class for the test dataset.\n",
    "        validation_dataset (Dataset): Dataset Class for the validation dataset.\n",
    "        train_dataloader (DataLoader): The train dataloader.\n",
    "        test_dataloader (DataLoader): The test dataloader.\n",
    "        validation_dataloader (DataLoader): The validation dataloader.\n",
    "        feature_scaler (MinMaxScaler): The scaler used to scale the features of the model input.\n",
    "        label_scaler (MinMaxScaler): The scaler used to scale the labels of the model input.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        not root_data_dir or not data_file_path or not data_splits_dir\n",
    "    ):  # Check for empty strings at the beginning\n",
    "        raise ValueError(\"File and directory paths cannot be empty strings.\")\n",
    "    DATA_ROOT = Path(root_data_dir)\n",
    "\n",
    "    DATA_CLEAN_PATH = DATA_ROOT / data_file_path  # Set the path to the complete dataset\n",
    "\n",
    "    if DATA_CLEAN_PATH.exists():\n",
    "        logger.info(f\"CSV file detected, reading from '{DATA_ROOT}'\")\n",
    "        df = pd.read_csv(\n",
    "            DATA_CLEAN_PATH, dtype=\"float32\"\n",
    "        )  # Convert data to float32 instead of, float64\n",
    "    else:\n",
    "        logger.info(f\"Downloading CSV file from '{dataset_url}'\\nand saving into '{DATA_ROOT}'\")\n",
    "        try:\n",
    "            os.makedirs(DATA_ROOT, exist_ok=True)  # Create the Data Root Directory\n",
    "            # Download and read the data into a pandas dataframe\n",
    "            df = pd.read_csv(\n",
    "                dataset_url, dtype=\"float32\"\n",
    "            )  # Convert data to float32 instead of, float64\n",
    "\n",
    "            # Clean the data before saving\n",
    "            try:\n",
    "                df = clean_data(df, logger)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"An unexpected error occurred cleaning the dataset:\\n{e}\")\n",
    "\n",
    "            df.to_csv(DATA_CLEAN_PATH, index=False)  # Save the file, omitting saving the row index\n",
    "        except OSError as e:\n",
    "            raise RuntimeError(f\"OS error occurred: {e}\")\n",
    "        except ParserError:\n",
    "            raise RuntimeError(f\"Failed to parse CSV from '{dataset_url}'\")\n",
    "        except ValueError as e:\n",
    "            raise RuntimeError(f\"Data cleaning error:\\n{e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"An unexpected error occurred when downloading or saving the \"\n",
    "                f\"dataset from '{dataset_url}' to '{DATA_CLEAN_PATH}':\\n{e}\"\n",
    "            )\n",
    "\n",
    "    # Define the paths for the data splits and scalers\n",
    "    DATA_SPLITS_DIR = DATA_ROOT / data_splits_dir\n",
    "    SCALER_DIR = DATA_ROOT / scaler_dir\n",
    "\n",
    "    TRAIN_DATA_PATH = DATA_SPLITS_DIR / \"train.csv\"\n",
    "    TEST_DATA_PATH = DATA_SPLITS_DIR / \"test.csv\"\n",
    "    VALIDATION_DATA_PATH = DATA_SPLITS_DIR / \"val.csv\"\n",
    "\n",
    "    FEATURE_SCALER_PATH = SCALER_DIR / \"feature-scaler.joblib\"\n",
    "    LABEL_SCALER_PATH = SCALER_DIR / \"label-scaler.joblib\"\n",
    "\n",
    "    # Dictate whether to use label scaler\n",
    "    USE_LABEL_SCALER = False\n",
    "\n",
    "    # Define the columns to drop from the features\n",
    "    columns_to_drop = [target_column] + extra_dropped_columns\n",
    "\n",
    "    # Define the Data Splits\n",
    "    TRAIN_SPLIT_PERCENTAGE = 0.9\n",
    "    VALIDATION_SPLIT_PERCENTAGE = 0.5\n",
    "\n",
    "    if (\n",
    "        os.path.exists(TRAIN_DATA_PATH)\n",
    "        and os.path.exists(TEST_DATA_PATH)\n",
    "        and os.path.exists(VALIDATION_DATA_PATH)\n",
    "    ):\n",
    "        logger.info(\n",
    "            f\"Train, Test, and Validation CSV datasets detected in '{DATA_SPLITS_DIR}.' Skipping generation and loading scaler(s)\"\n",
    "        )\n",
    "        try:\n",
    "            feature_scaler = joblib.load(FEATURE_SCALER_PATH)\n",
    "            logger.info(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            if USE_LABEL_SCALER:\n",
    "                joblib.dump(\n",
    "                    label_scaler, LABEL_SCALER_PATH\n",
    "                )  # Not used for this classification task\n",
    "                logger.info(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "            else:\n",
    "                label_scaler = None  # Omit the label scaler loading\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Scaler file not found: {e}\")\n",
    "        except EOFError as e:\n",
    "            raise RuntimeError(f\"Scaler file appears to be empty or corrupted: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when loading scalers: {e}\")\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Datasets not found in '{DATA_SPLITS_DIR}' or incomplete. Generating datasets...\"\n",
    "        )\n",
    "        # os.makedirs(MODEL_ROOT, exist_ok=True)\n",
    "        os.makedirs(DATA_SPLITS_DIR, exist_ok=True)  # Create the Data Splits Parent Directory\n",
    "        os.makedirs(SCALER_DIR, exist_ok=True)  # Create the Scaler Parent Directory\n",
    "\n",
    "        # Create the scaler objects\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        if USE_LABEL_SCALER:\n",
    "            label_scaler = MinMaxScaler()\n",
    "        else:\n",
    "            label_scaler = None  # Not used for this Classification task\n",
    "\n",
    "        try:\n",
    "            df_features = df.drop(columns=columns_to_drop, inplace=False)\n",
    "            df_labels = df[\n",
    "                [target_column]\n",
    "            ]  # Instead of returning a pandas Series using \"[]\", return a dataframe using the \"[[]]\" to get a shape with (-1,1)\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"One or more specified columns to drop do not exist in the DataFrame: {e}\"\n",
    "            )\n",
    "\n",
    "        # Split into smaller DataFrames for the Train, Test, and Validation splits\n",
    "        X_train, X_inter, Y_train, Y_inter = train_test_split(\n",
    "            df_features, df_labels, test_size=1 - TRAIN_SPLIT_PERCENTAGE, random_state=42\n",
    "        )\n",
    "        X_validation, X_test, Y_validation, Y_test = train_test_split(\n",
    "            X_inter, Y_inter, test_size=1 - VALIDATION_SPLIT_PERCENTAGE, random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit the scalers to the data\n",
    "        feature_scaler.fit(X_train)\n",
    "        # Only scale the labels if required\n",
    "        if USE_LABEL_SCALER:\n",
    "            label_scaler.fit(Y_train)  # Not used for this Classification task\n",
    "\n",
    "        # Save the fitted scaler object\n",
    "        try:\n",
    "            joblib.dump(feature_scaler, FEATURE_SCALER_PATH)\n",
    "            logger.info(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            # Save the Label Scaler if utilized\n",
    "            if USE_LABEL_SCALER:\n",
    "                joblib.dump(\n",
    "                    label_scaler, LABEL_SCALER_PATH\n",
    "                )  # Not used for this Classification task\n",
    "                logger.info(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Save path not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when saving  Scaler(s): {e}\")\n",
    "\n",
    "        # Scale the rest of the data; returns numpy arrays\n",
    "        X_train_scaled = feature_scaler.transform(X_train)\n",
    "        X_validation_scaled = feature_scaler.transform(X_validation)\n",
    "        X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "        if USE_LABEL_SCALER:  # Not used for this Classification task\n",
    "            Y_train = label_scaler.transform(Y_train)\n",
    "            Y_validation = label_scaler.transform(Y_validation)\n",
    "            Y_test = label_scaler.transform(Y_test)\n",
    "\n",
    "        logger.info(f\"Train Features (Scaled) Shape: {X_train_scaled.shape}\")\n",
    "        logger.info(f\"Validation Features (Scaled) Shape: {X_validation_scaled.shape}\")\n",
    "        logger.info(f\"Test Features (Scaled) Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        if USE_LABEL_SCALER:\n",
    "            logger.info(f\"Train Labels (Scaled) Shape: {Y_train.shape}\")\n",
    "            logger.info(f\"Validation Labels (Scaled) Shape: {Y_validation.shape}\")\n",
    "            logger.info(f\"Test Labels (Scaled) Shape: {Y_test.shape}\")\n",
    "        else:\n",
    "            logger.info(f\"Train Labels Shape: {Y_train.shape}\")\n",
    "            logger.info(f\"Validation Labels Shape: {Y_validation.shape}\")\n",
    "            logger.info(f\"Test Labels Shape: {Y_test.shape}\")\n",
    "\n",
    "        # Define the column names of the features and label\n",
    "        features_names = df_features.columns\n",
    "        label_name = df_labels.columns\n",
    "\n",
    "        # Create dataframes using the scaled data\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=features_names)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=features_names)\n",
    "        X_validation_df = pd.DataFrame(X_validation_scaled, columns=features_names)\n",
    "        Y_train_df = pd.DataFrame(Y_train, columns=label_name)\n",
    "        Y_test_df = pd.DataFrame(Y_test, columns=label_name)\n",
    "        Y_validation_df = pd.DataFrame(Y_validation, columns=label_name)\n",
    "\n",
    "        # Concatenate the features and labels back into a single DataFrame for each set\n",
    "        train_data_frame = pd.concat([X_train_df, Y_train_df.reset_index(drop=True)], axis=1)\n",
    "        test_data_frame = pd.concat([X_test_df, Y_test_df.reset_index(drop=True)], axis=1)\n",
    "        validation_data_frame = pd.concat(\n",
    "            [X_validation_df, Y_validation_df.reset_index(drop=True)], axis=1\n",
    "        )\n",
    "\n",
    "        # Saving the split data to csv files\n",
    "        try:\n",
    "            train_data_frame.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "            test_data_frame.to_csv(TEST_DATA_PATH, index=False)\n",
    "            validation_data_frame.to_csv(VALIDATION_DATA_PATH, index=False)\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Save path not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"An unexpected error occurred when saving datasets to CSV files:\\n{e}\"\n",
    "            )\n",
    "\n",
    "    # Creating Datasets from the stored datasets\n",
    "    logger.info(f\"INITIALIZING DATASETS\")\n",
    "    train_dataset = CustomDataset(csv_file=TRAIN_DATA_PATH, label_column=target_column)\n",
    "    test_dataset = CustomDataset(csv_file=TEST_DATA_PATH, label_column=target_column)\n",
    "    val_dataset = CustomDataset(csv_file=VALIDATION_DATA_PATH, label_column=target_column)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Creating DataLoaders with 'batch_size'=({batch_size}), 'num_workers'=({num_workers}), 'pin_memory'=({pin_memory}). Training dataset 'drop_last'=({drop_last})\"\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Training DataLoader has ({len(train_dataloader)}) batches, Test DataLoader has ({len(test_dataloader)}) batches, Validation DataLoader has ({len(validation_dataloader)}) batches\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"==================================================================\")\n",
    "    for name, dataloader in [\n",
    "        (\"Train\", train_dataloader),\n",
    "        (\"Validation\", validation_dataloader),\n",
    "        (\"Test\", test_dataloader),\n",
    "    ]:\n",
    "        features, labels = next(iter(dataloader))  # Get one batch\n",
    "\n",
    "        logger.info(f\"{name} Dataloader Batch Information\")\n",
    "        logger.info(f\"Features Shape: '{features.shape}' |  DataTypes: '{features.dtype}'\")\n",
    "        logger.info(f\"Labels Shape: '{labels.shape}'   |  DataTypes: '{labels.dtype}' \")\n",
    "        logger.info(\"==================================================================\")\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        val_dataset,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        feature_scaler,\n",
    "        label_scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e203af",
   "metadata": {},
   "source": [
    "## Agent Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e22b8",
   "metadata": {},
   "source": [
    "### Module Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "955cfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleLayer(torch.nn.Module):\n",
    "    \"\"\"Class for the individual layer blocks.\"\"\"\n",
    "\n",
    "    def __init__(self, intermediate_dim=32, dropout_rate=0.1):\n",
    "        \"\"\"Initializer for the 'ModuleLayer' class.\n",
    "        Args:\n",
    "            intermediate_dim (int): The dimension of the intermediate layer.\n",
    "            dropout_rate (float): The dropout rate to apply after the ReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mod_linear = torch.nn.Linear(intermediate_dim, intermediate_dim)\n",
    "        self.mod_norm = torch.nn.LayerNorm(normalized_shape=intermediate_dim)\n",
    "        self.mod_relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the layer block.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor passing the input through the layer operations.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.mod_linear(x)\n",
    "        x = self.mod_norm(x)\n",
    "        x = self.mod_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9c2e5",
   "metadata": {},
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0898fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    \"\"\"Class for Agent Structure using multiple Layer Blocks.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"Initializer for the 'Agent' class.\n",
    "        Args:\n",
    "            cfg (dict): Configuration dictionary containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(\n",
    "            in_features=cfg[\"in_dim\"], out_features=cfg[\"intermediate_dim\"]\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            *[\n",
    "                ModuleLayer(\n",
    "                    intermediate_dim=cfg[\"intermediate_dim\"], dropout_rate=cfg[\"dropout_rate\"]\n",
    "                )\n",
    "                for _ in range(int(cfg[\"num_blocks\"]))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.out = torch.nn.Linear(in_features=cfg[\"intermediate_dim\"], out_features=cfg[\"out_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the Agent's Layers.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        Returns:\n",
    "            x (torch.Tensor): Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = self.linear(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44d5f8",
   "metadata": {},
   "source": [
    "### Evaluate Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "58184545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    logger: Logger,\n",
    "    model: Module,\n",
    "    dataloader: DataLoader,\n",
    "    training_config: dict,\n",
    "    current_epoch: int = None,\n",
    "    max_epochs: int = None,\n",
    "    device: str = \"cpu\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset and returns the average loss.\n",
    "    Args:\n",
    "        logger (Logger): The logger instance to log messages.\n",
    "        model (Module): The Model.\n",
    "        dataloader (DataLoader): The dataloader to calculate average loss with.\n",
    "        training_config (dict): The base configurations used for training the model; now for evaluation.\n",
    "        current_epoch (int): The current epoch [optional].\n",
    "        max_epochs (int): The maximum number of epochs [optional].\n",
    "        device (str): The device that the calculations will take place on.\n",
    "    Returns:\n",
    "        avg_loss (float): The calculated average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    loss_choice = training_config.get(\"loss_function\", \"mae\").lower()\n",
    "\n",
    "    # loss_fn  ==>  Use reduction='sum' instead of 'mean' for total loss\n",
    "    if loss_choice == \"mae\":\n",
    "        loss_fn = torch.nn.L1Loss(reduction=\"sum\")  # Define the Loss function\n",
    "    elif loss_choice == \"crossentropyloss\":\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")  # Define the Loss function\n",
    "    # Elif more loss functions are added in the future, they can be added here\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss_choice}\")\n",
    "\n",
    "    if len(dataloader.dataset) == 0:\n",
    "        logger.warning(\"Warning: Evaluation dataset is empty. Skipping evaluation.\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = loss_fn(outputs, batch_targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)  # Calculate the average loss on the dataset\n",
    "\n",
    "    if current_epoch and max_epochs:  # If the function was called in the training loop\n",
    "        logger.info(\n",
    "            f\"===================  [Epoch ({current_epoch}/{max_epochs})]  ===================\"\n",
    "        )\n",
    "        logger.info(f\"Entire Validation Dataset Average Loss: {avg_loss:.4f}\")\n",
    "        logger.info(f\"====================================================\")\n",
    "\n",
    "    else:  # If the function was called outside of the training loop\n",
    "        logger.info(f\"===============================================\")\n",
    "        logger.info(f\"Entire Dataset Average Loss: {avg_loss:.4f} \")\n",
    "        logger.info(f\"=====================================================\")\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd57a1c",
   "metadata": {},
   "source": [
    "### Close the logger object and exit. Used during Exception Handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8e00acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_log_with_exit(exit_code: int) -> None:\n",
    "    \"\"\"Exits the application with the given exit code.\n",
    "    Args:\n",
    "        exit_code (int): The exit code to return when exiting the application.\n",
    "    \"\"\"\n",
    "    # This line closes all handlers and releases the file lock\n",
    "    logging.shutdown()\n",
    "    exit(exit_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d54fb0",
   "metadata": {},
   "source": [
    "## Miscelaneous Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6668e",
   "metadata": {},
   "source": [
    "### Checks for current running mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa33b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook():\n",
    "    \"\"\"Checks if the code is running in a Jupyter notebook environment.\n",
    "    Returns:\n",
    "        bool: True if running in a Jupyter notebook, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "\n",
    "        # print(f\"Detected shell: {shell}\", flush=True)\n",
    "\n",
    "        if shell == \"ZMQInteractiveShell\":\n",
    "            return True  # Jupyter notebook or qtconsole\n",
    "        elif shell == \"TerminalInteractiveShell\":\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other types\n",
    "    except NameError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c2442",
   "metadata": {},
   "source": [
    "### Creating Logger Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4fa1be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(config: dict, propogate: bool = False) -> Logger:\n",
    "    \"\"\"Sets up and returns a named logger based on the provided config dictionary. The new logger will have different handlers based on the config.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Dictionary containing logging configuration.\n",
    "        propogate (bool): Whether to allow log messages to propagate to ancestor loggers.\n",
    "    Returns:\n",
    "        Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "\n",
    "    logger_name = config.get(\"logger_name\", \"main\")\n",
    "    log_to_file = config.get(\"log_to_file\", True)  # Set whether to log to a logfile or not\n",
    "    log_file = config.get(\"log_file\", \"logs/app.log\")  # Get the log file path\n",
    "    log_lvl = config.get(\"log_level\", \"INFO\")\n",
    "    log_level = getattr(logging, log_lvl.upper(), logging.INFO)  # Set fallback if invalid input\n",
    "    log_mode = config.get(\"log_mode\", \"w\")  # Set the log file mode\n",
    "    log_format = config.get(\"log_format\", \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    date_format = config.get(\"date_format\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    log_to_console = config.get(\"log_to_console\", True)  # Set whether to log to console or not\n",
    "\n",
    "    handlers = []  # Initialize the list of logging handlers\n",
    "\n",
    "    logger = logging.getLogger(logger_name)  # Create logger object with the specified name\n",
    "\n",
    "    if not log_to_file and not log_to_console:\n",
    "        # If no handlers are specified by the config\n",
    "        print(\n",
    "            f\"Warning: No logging handlers configured for {logger_name}.\\nVerbose Logging will be disabled.\\nIn 'config/config.json', set ['log_to_file': true] or ['log_to_console': true] if you want to change the logging behavior.\",\n",
    "            flush=True,\n",
    "        )\n",
    "    else:\n",
    "        # Create log parent directory if it doesn't exist\n",
    "        parent_dir = os.path.dirname(log_file)  # Get the parent directory of the log file\n",
    "        if parent_dir and parent_dir != \".\":\n",
    "            try:\n",
    "                os.makedirs(name=parent_dir, exist_ok=True)\n",
    "                print(\n",
    "                    f\"Parent directory '{parent_dir}' used to store the log file.\", flush=True\n",
    "                )  # flush=True to ensure the message is printed immediately\n",
    "            except OSError as e:\n",
    "                print(\n",
    "                    f\"Error creating directory '{parent_dir}': {e} INFO: Using default log file 'app.log' instead.\",\n",
    "                    flush=True,\n",
    "                )\n",
    "                log_file = \"app.log\"  # Fall back to a default log file if problem occurs.\n",
    "\n",
    "        # Remove all old handlers inherrited from the root logger\n",
    "        for handler in logger.handlers[:]:\n",
    "            handler.close()\n",
    "            logger.removeHandler(handler)\n",
    "\n",
    "        formatter = logging.Formatter(\n",
    "            fmt=log_format, datefmt=date_format\n",
    "        )  # Create a formatter for the log messages\n",
    "\n",
    "        if log_to_console:\n",
    "            console_handler = (\n",
    "                logging.StreamHandler()\n",
    "            )  # Initialize sending log messages to the console (stdout)\n",
    "            console_handler.setFormatter(formatter)  # Set the formatter for the console handler\n",
    "            handlers.append(console_handler)  # Add the console_handler to the list of handlers\n",
    "        if log_to_file:\n",
    "            file_handler = logging.FileHandler(\n",
    "                filename=log_file, mode=log_mode\n",
    "            )  # Initialize sending log messages to a file\n",
    "            file_handler.setFormatter(formatter)  # Set the style for the console handler\n",
    "            handlers.append(file_handler)  # Add the file_handler to the list of handlers\n",
    "\n",
    "        # Add the handlers to the logger\n",
    "        for handler in handlers:\n",
    "            logger.addHandler(handler)\n",
    "\n",
    "    logger.setLevel(log_level)  # Set logger minimum log level\n",
    "\n",
    "    logger.propagate = propogate  # Prevent the log messages from being propagated to the root logger; gets rid of the root logger's default handlers,\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146caffd",
   "metadata": {},
   "source": [
    "### Retrieve Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c64d172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_logger(name: str = \"root\") -> Logger:\n",
    "    \"\"\"\n",
    "    Retrieves a named logger. If no handlers are attached, returns a root logger instance.\n",
    "    Args:\n",
    "        name (str): The name of the logger to retrieve.\n",
    "    Returns:\n",
    "        Logger: The logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        print(f\"Retrieving root logger.\", flush=True)\n",
    "        return logging.getLogger()\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17deed",
   "metadata": {},
   "source": [
    "### Close file handlers and exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3fa84e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_and_exit(logger: Logger, exit_code: int) -> None:\n",
    "    \"\"\"Closes all handlers of the logger and exits the program with the given exit code.\n",
    "    Args:\n",
    "        logger (Logger): The logger instance to close.\n",
    "        exit_code (int): The exit code to terminate the program with.\n",
    "    \"\"\"\n",
    "    print(\"Note: Closing any named loggers...\", flush=True)\n",
    "    used_handlers = logger.handlers[:]\n",
    "    for handler in used_handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    print(\"Note: Closing any root logger...\", flush=True)\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        handler.close()\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    print(f\"Exiting program with exit code {exit_code}.\", flush=True)\n",
    "\n",
    "    if is_notebook():\n",
    "        print(\n",
    "            \"Detected Jupyter Notebook environment. Skipping sys.exit() to avoid kernel interruption.\",\n",
    "            flush=True,\n",
    "        )\n",
    "    else:\n",
    "        sys.exit(exit_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72db04b",
   "metadata": {},
   "source": [
    "## Sample Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ee757d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(\n",
    "    logger, model, dataset, training_config, target_scaler=None, feature_scaler=None, samples=5\n",
    ") -> None:\n",
    "    \"\"\"Function to sample a model on a given dataset and provide a detailed ouput of the predictions, actual values, and loss for each sample.\n",
    "    Args:\n",
    "        logger (Logger): The logger instance to log messages.\n",
    "        model (Agent): The Model.\n",
    "        dataset (Dataset): The dataset to sample from.\n",
    "        training_config (dict): The base configurations used for training the model; now for evaluation.\n",
    "        target_scaler (sklearn.base.TransformerMixin): A scikit-learn scaler instance (e.g., StandardScaler, MinMaxScaler) used to scale the target column of the dataset.\n",
    "        feature_scaler (sklearn.base.TransformerMixin): A scikit-learn scaler instance (e.g., StandardScaler, MinMaxScaler) used to scale the features of the dataset.\n",
    "        samples (int): The number of samples to take from the dataset. If 0 or less, defaults to 5. If more than dataset size, adjusts to dataset size.\n",
    "    \"\"\"\n",
    "    loss_choice = training_config.get(\"loss_function\", \"mae\").lower()\n",
    "\n",
    "    if loss_choice == \"mae\":\n",
    "        loss_fn = torch.nn.L1Loss(reduction=\"sum\")  # Define the Loss function\n",
    "    elif loss_choice == \"crossentropyloss\":\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")  # Define the Loss function\n",
    "    # Elif more loss functions are added in the future, they can be added here\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss_choice}\")\n",
    "\n",
    "    # Sample size validation\n",
    "    if samples <= 0:\n",
    "        logger.info(\"Number of samples must be greater than zero. Setting samples to 5\")\n",
    "        samples = 5\n",
    "\n",
    "    if samples > len(dataset):\n",
    "        logger.info(\n",
    "            f\"Requested number of samples ({samples}) exceeds dataset size ({len(dataset)}). Adjusting to dataset size.\"\n",
    "        )\n",
    "        samples = len(dataset)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for index in range(samples):\n",
    "            # Think about randomizing this\n",
    "            (features, target) = dataset[index]\n",
    "\n",
    "            # DEBUG\n",
    "            # print(features[:5])   # Print the first 5 features for inspection\n",
    "\n",
    "            if loss_choice == \"mae\":\n",
    "                pred = model(features)  # Get the models prediction baesd on the input features\n",
    "            if loss_choice == \"crossentropyloss\":\n",
    "                pred = model(features)\n",
    "                tensor_pred = pred.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # DEBUG\n",
    "                # logger.info(f\"Class Probabilities Tensor: {tensor_pred}, Size: {tensor_pred.size()}\")\n",
    "\n",
    "                pred_label = torch.argmax(tensor_pred, dim=1).item()  # Get the prediction\n",
    "            loss = loss_fn(\n",
    "                pred, target\n",
    "            )  # Calculate the loss, adding an extra dimension to the target to match the prediction shape\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Un-normalize agent prediction and target using the inverse function and get the scalar value from the numpy arrays\n",
    "            if target_scaler:\n",
    "                pred_unscaled = target_scaler.inverse_transform(pred.cpu().numpy().reshape(-1, 1))[\n",
    "                    0, 0\n",
    "                ]\n",
    "\n",
    "                target_unscaled = target_scaler.inverse_transform(\n",
    "                    target.cpu().numpy().reshape(-1, 1)\n",
    "                )[0, 0]\n",
    "\n",
    "            # UN-NORMALIZE FEATURES IF SCALER IS PROVIDED\n",
    "            if feature_scaler:\n",
    "                features_unscaled = feature_scaler.inverse_transform(\n",
    "                    features.cpu().numpy().reshape(1, -1)\n",
    "                )[0]\n",
    "                logger.info(f\"Un-normalized features: {features_unscaled}\")\n",
    "            else:\n",
    "                logger.info(f\"Normalized features: {features.numpy()}\")\n",
    "\n",
    "            # SHOW MODEL PREDICTIONS BASED ON THE LOSS CRITERION\n",
    "            if loss_choice == \"mae\":\n",
    "                logger.info(\n",
    "                    f\"Model predicts: {pred} | Actual: {target:.7f} | Loss: {loss.item():.7f}\"\n",
    "                )\n",
    "            if loss_choice == \"crossentropyloss\":\n",
    "                logger.info(\n",
    "                    f\"Model predicts: {pred_label} | Actual: {target} | Loss: {loss.item():.7f}\"\n",
    "                )\n",
    "\n",
    "            # SHOW THE PREDICTIONS UN-NORMALIZED IF A LABEL SCALER WAS USED\n",
    "            if target_scaler:\n",
    "                logger.info(\n",
    "                    f\"Un-normalized prediction: {pred_unscaled:.4f} | Un-normalized actual: {target_unscaled:.4f}\\n\"\n",
    "                )\n",
    "\n",
    "            logger.info(\"===========================================\\n\")\n",
    "    avg_loss = total_loss / samples\n",
    "    logger.info(f\"Average loss over the samples: {avg_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9fb08",
   "metadata": {},
   "source": [
    "## Begin Sampling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "973c3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --- Load Config File ---\n",
    "try:\n",
    "    with open(file=CONFIG_PATH, mode=\"r\") as f:\n",
    "        global_config = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Config file not found. Please ensure '{CONFIG_PATH}' exists. Modify 'CONFIG_PATH' in Global Variables section if needed.\",\n",
    "        flush=True,\n",
    "    )\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error loading config file: {e}. Exiting.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3776fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logging Initialization Section ---\n",
    "log_to_console = global_config[\"logging\"][\"log_to_console\"]  # Set whether to log to console or not\n",
    "log_to_file = global_config[\"logging\"][\"log_to_file\"]  # Set whether to log to a logfile or not\n",
    "logger_config = global_config[\"logging\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "57e34b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory 'logs' used to store the log file.\n"
     ]
    }
   ],
   "source": [
    "# Configure the root logger for any backup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.CRITICAL\n",
    ")  # Set root logger to highest level to suppress unwanted logs\n",
    "\n",
    "# Create the named logger only if the user wants to log to console or file\n",
    "if log_to_console or log_to_file:\n",
    "    logger = setup_logger(config=logger_config, propogate=False)\n",
    "# Check if the user disabled both logging methods and resort to the root logger with no handlers\n",
    "elif not log_to_file and not log_to_console:\n",
    "    print(\n",
    "        f\"========================================\\nWarning: No logging handlers configured for logger, using root logger.\\nVerbose Logging will be disabled.\\nIf you want to change the logging behavior:\\nIn 'config/config.json', set ['log_to_file': true] or ['log_to_console': true]\\n========================================\\n \",\n",
    "        flush=True,\n",
    "    )\n",
    "    logger = logging.getLogger()  # Use the root logger\n",
    "\n",
    "    # Remove all handlers inherrited from the root logger, if any exist\n",
    "    for handler in logger.handlers[:]:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e9c09a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = retrieve_logger(global_config[\"logging\"][\"logger_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c8ace3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "BASE_CONFIG = global_config[\"model\"]\n",
    "TRAINING_CONFIG = global_config[\"training\"]\n",
    "PIPELINE_CONFIG = global_config[\"data\"]\n",
    "PARSER_ARGS = global_config[\"parser_defaults\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "377cd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVED_LOCATION = \"../models/my-model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ec2bd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_policy = Agent(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "619518e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load(f=MODEL_SAVED_LOCATION, weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ea52cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3374f8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (linear): Linear(in_features=9, out_features=128, bias=True)\n",
       "  (layers): Sequential(\n",
       "    (0): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): ModuleLayer(\n",
       "      (mod_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mod_relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "90559fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:22:53 - main - INFO - CSV file detected, reading from 'Data'\n",
      "2025-10-31 10:22:54 - main - INFO - Train, Test, and Validation CSV datasets detected in 'Data\\DataSplits.' Skipping generation and loading scaler(s)\n",
      "2025-10-31 10:22:54 - main - INFO - Feature scaler stored in: (Data\\Scalers\\feature-scaler.joblib)\n",
      "2025-10-31 10:22:54 - main - INFO - INITIALIZING DATASETS\n",
      "2025-10-31 10:22:54 - main - INFO - Creating DataLoaders with 'batch_size'=(64), 'num_workers'=(0), 'pin_memory'=(False). Training dataset 'drop_last'=(True)\n",
      "2025-10-31 10:22:54 - main - INFO - Training DataLoader has (14745) batches, Test DataLoader has (819) batches, Validation DataLoader has (819) batches\n",
      "2025-10-31 10:22:54 - main - INFO - ==================================================================\n",
      "2025-10-31 10:22:54 - main - INFO - Train Dataloader Batch Information\n",
      "2025-10-31 10:22:54 - main - INFO - Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "2025-10-31 10:22:54 - main - INFO - Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "2025-10-31 10:22:54 - main - INFO - ==================================================================\n",
      "2025-10-31 10:22:54 - main - INFO - Validation Dataloader Batch Information\n",
      "2025-10-31 10:22:54 - main - INFO - Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "2025-10-31 10:22:54 - main - INFO - Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "2025-10-31 10:22:54 - main - INFO - ==================================================================\n",
      "2025-10-31 10:22:54 - main - INFO - Test Dataloader Batch Information\n",
      "2025-10-31 10:22:54 - main - INFO - Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "2025-10-31 10:22:54 - main - INFO - Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "2025-10-31 10:22:54 - main - INFO - ==================================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Use dictionary unpacking to pass the PIPELINE_CONFIG parameters to the data_pipeline function\n",
    "    (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        validation_dataset,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        feature_scaler,\n",
    "        label_scaler,\n",
    "    ) = data_pipeline(\n",
    "        logger=logger,\n",
    "        **PIPELINE_CONFIG,\n",
    "        batch_size=PARSER_ARGS[\"dataloader_batch_size\"],\n",
    "        num_workers=PARSER_ARGS[\"dataloader_num_workers\"],\n",
    "        pin_memory=PARSER_ARGS[\"dataloader_pin_memory\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    logger.error(f\"Caught a 'value' error: {e}\", exc_info=True, stack_info=True)\n",
    "except RuntimeError as e:\n",
    "    logger.error(f\"Caught a 'runtime' error: {e}\", exc_info=True, stack_info=True)\n",
    "except MemoryError as e:\n",
    "    logger.error(\n",
    "        f\"Memory Error: {e}. Consider reducing the DataLoader's batch size or model complexity.\",\n",
    "        exc_info=True,\n",
    "        stack_info=True,\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    logger.error(\n",
    "        \"Training interrupted by user (KeyboardInterrupt).\", exc_info=True, stack_info=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"An unexpected error occurred during model training: {e}\",\n",
    "        exc_info=True,\n",
    "        stack_info=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b979b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING THE TRAINED POLICY:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:25:02 - main - INFO - ===============================================\n",
      "2025-10-31 10:25:02 - main - INFO - Entire Dataset Average Loss: 0.0191 \n",
      "2025-10-31 10:25:02 - main - INFO - =====================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Testing Trained Model ---\n",
    "print(\"\\nTESTING THE TRAINED POLICY:\")\n",
    "test_loss = evaluate_model(\n",
    "    logger=logger,\n",
    "    model=trained_policy,\n",
    "    dataloader=test_dataloader,\n",
    "    training_config=TRAINING_CONFIG,\n",
    "    current_epoch=None,\n",
    "    max_epochs=None,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5ea82cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019055956411558144\n"
     ]
    }
   ],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bd35f4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 1.1000000e+01  8.1400003e+00  1.0000000e+00  7.0000000e+00\n",
      "  3.8936699e+01 -7.6994003e+01  6.0172300e+05  3.9608166e+01\n",
      " -7.7142876e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0001433\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 10.        91.83       0.        16.        38.5957   -99.55399\n",
      " 320.        39.450687 -99.98159 ]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0002020\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 2.0000000e+00  7.3510002e+01  1.0000000e+00  2.0000000e+01\n",
      "  3.9029800e+01 -7.7079300e+01  1.9054000e+04  4.0004333e+01\n",
      " -7.6774902e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0109874\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 7.000000e+00  7.561000e+01  1.000000e+00  4.500000e+01  3.884320e+01\n",
      " -7.860030e+01  6.018000e+03  3.870850e+01 -7.925962e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0004813\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 12.       127.96001    1.        34.        42.4069   -74.1528\n",
      " 277.        41.695232 -74.11219 ]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0001708\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 1.2000000e+01  3.7100003e+00  1.0000000e+00  4.8000000e+01\n",
      "  4.2967602e+01 -8.8043404e+01  8.1731200e+05  4.2321136e+01\n",
      " -8.7781647e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0001039\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [  5.        32.19       0.         2.        35.1791   -91.2594\n",
      "  37.        35.344135 -91.48959 ]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0006092\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [  2.       67.7       0.       38.       39.9914  -80.4408  723.99994\n",
      "  40.61358 -80.4048 ]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0026909\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 1.0000000e+01  3.8130001e+01  1.0000000e+00  1.2000000e+01\n",
      "  4.2851101e+01 -9.3620003e+01  3.0320000e+03  4.2535946e+01\n",
      " -9.4357559e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0001861\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 6.0000000e+00  1.5049999e+01  1.0000000e+00  1.8000000e+01\n",
      "  2.9991199e+01 -9.0247902e+01  6.8211000e+04  3.0200298e+01\n",
      " -9.1040871e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0005682\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [   2.         51.1         0.         31.         40.2367    -74.0067\n",
      " 1533.0001     39.813843  -74.194466]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0028390\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 1.0000000e+01  2.4000000e+01  0.0000000e+00  1.4000000e+01\n",
      "  4.1537800e+01 -8.8057198e+01  1.2835400e+05  4.1654194e+01\n",
      " -8.8771461e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0001699\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 4.000000e+00  7.054000e+01  0.000000e+00  9.000000e+00  2.669390e+01\n",
      " -8.194520e+01  1.563910e+05  2.683965e+01 -8.107210e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0010359\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 2.000000e+00  5.947000e+01  1.000000e+00  1.000000e+00  3.346290e+01\n",
      " -8.679040e+01  4.938060e+05  3.319760e+01 -8.599377e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0049837\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 2.0000000e+00  3.7970001e+01  0.0000000e+00  1.0000000e+00\n",
      "  3.2617599e+01 -8.6947502e+01  1.4120000e+03  3.2502003e+01\n",
      " -8.6172707e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0035788\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 1.000000e+00  4.350000e+00  0.000000e+00  1.900000e+01  4.210010e+01\n",
      " -7.336110e+01  2.121000e+03  4.146934e+01 -7.322279e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0037329\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 6.0000000e+00  5.5200001e+01  1.0000000e+00  3.2000000e+01\n",
      "  3.5986599e+01 -1.0606540e+02  1.8408000e+04  3.6495747e+01\n",
      " -1.0610172e+02]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0005500\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 2.000000e+00  5.064000e+01  1.000000e+00  4.500000e+01  3.864760e+01\n",
      " -7.877170e+01  4.367000e+03  3.857418e+01 -7.894774e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0133992\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 4.0000000e+00  1.6958000e+02  0.0000000e+00  1.3000000e+01\n",
      "  4.3649799e+01 -1.1643060e+02  8.4106000e+04  4.4597248e+01\n",
      " -1.1735934e+02]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0010679\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [  2.        68.33       0.        23.        48.6031   -93.2977\n",
      " 136.        48.987617 -93.56081 ]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0024117\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 6.0000000e+00  1.8635001e+02  1.0000000e+00  3.5000000e+01\n",
      "  4.1069500e+01 -8.1548798e+01  2.7213400e+05  4.0342529e+01\n",
      " -8.0756226e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0011700\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [   4.       113.84       0.        20.        39.0026   -76.1424\n",
      " 3862.        38.24986  -76.15953]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0011705\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 6.0000000e+00  1.8250000e+01  1.0000000e+00  1.2000000e+01\n",
      "  4.2851101e+01 -9.3620003e+01  3.0320000e+03  4.2165337e+01\n",
      " -9.3345909e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0005280\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [  11.         86.56        1.         50.         41.0552   -110.16031\n",
      "   49.000004   41.173046 -109.430725]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0001629\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 8.0000000e+00  5.5799999e+00  1.0000000e+00  3.1000000e+01\n",
      "  4.0333000e+01 -7.3980904e+01  3.2790000e+03  4.1229465e+01\n",
      " -7.3495323e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0003095\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 1.2000000e+01  3.3399999e+00  0.0000000e+00  3.4000000e+01\n",
      "  4.0826500e+01 -7.3938301e+01  1.5773850e+06  4.0588177e+01\n",
      " -7.4142982e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0000977\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [  5.         7.31       1.        43.        28.7724   -96.4793\n",
      " 911.00006   28.55613  -96.752556]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0007144\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 6.0000000e+00  1.6639999e+01  0.0000000e+00  2.6999998e+01\n",
      "  3.6201698e+01 -8.1128601e+01  2.1134000e+04  3.5770763e+01\n",
      " -8.1930443e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0004842\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [ 1.2999999e+01  5.4499998e+00  1.0000000e+00  3.4000000e+01\n",
      "  4.1059200e+01 -7.3739502e+01  1.1250000e+04  4.1679688e+01\n",
      " -7.3429222e+01]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0000946\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Un-normalized features: [   2.         37.02        0.          4.         40.3406   -120.282394\n",
      "  104.         39.618164 -120.981575]\n",
      "2025-10-31 10:25:12 - main - INFO - Model predicts: 0 | Actual: 0 | Loss: 0.0026610\n",
      "2025-10-31 10:25:12 - main - INFO - ===========================================\n",
      "\n",
      "2025-10-31 10:25:12 - main - INFO - Average loss over the samples: 0.0019102\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sample_model(\n",
    "        logger,\n",
    "        trained_policy,\n",
    "        test_dataset,\n",
    "        TRAINING_CONFIG,\n",
    "        label_scaler,\n",
    "        feature_scaler,\n",
    "        samples=30,\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Caught exception during model sampling: \\n{e}\", exc_info=True, stack_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "70e89c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Closing any named loggers...\n",
      "Note: Closing any root logger...\n",
      "Exiting program with exit code 0.\n",
      "Detected Jupyter Notebook environment. Skipping sys.exit() to avoid kernel interruption.\n"
     ]
    }
   ],
   "source": [
    "# --- Closes any logger file handlers and exits the program ---\n",
    "close_and_exit(logger, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
