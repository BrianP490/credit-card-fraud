{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc715cf",
   "metadata": {},
   "source": [
    "# Cleaning Data and Creating DataLoader Function\n",
    "- Balanced out the dataset using IMBlearn's RandomOversampler\n",
    "- Look into using parquet files to shrink data files down (comes at the cost of not being human readable) - Not going to implement.\n",
    "- Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad075ef",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34672e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from importlib.metadata import version\n",
    "from logging import Logger\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db165c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6886c558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:pandas version: 2.3.2\n",
      "INFO:__main__:importlib-metadata version: 8.7.0\n",
      "INFO:__main__:pyarrow version: 21.0.0\n"
     ]
    }
   ],
   "source": [
    "packages = [\"pandas\", \"importlib-metadata\", \"pyarrow\"]\n",
    "for package in packages:\n",
    "    try:\n",
    "        logger.info(f\"{package} version: {version(package)}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not get version for package {package}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040322d",
   "metadata": {},
   "source": [
    "## Load Dataframe from csv file in local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c642d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../Data\")\n",
    "RAW_DATA_DIR_NAME = \"Downloaded-Data\"\n",
    "# DATA_RAW_FILE_NAME = \"credit-card-fraud.csv\"\n",
    "\n",
    "DATA_RAW_FILE_NAME = \"credit-card-fraud-RAW.csv\"\n",
    "DATA_CLEAN_FILE_NAME = \"credit-card-fraud-CLEAN.csv\"\n",
    "\n",
    "RAW_DATA_PATH = DATA_ROOT / RAW_DATA_DIR_NAME / DATA_RAW_FILE_NAME\n",
    "DATA_PATH = DATA_ROOT / RAW_DATA_DIR_NAME / DATA_CLEAN_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d47ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RAW_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50211765",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631a6e8",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3845851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these = [\n",
    "    \"Unnamed: 0\",\n",
    "    \"trans_date_trans_time\",\n",
    "    \"cc_num\",\n",
    "    \"merchant\",\n",
    "    \"first\",\n",
    "    \"last\",\n",
    "    \"street\",\n",
    "    \"city\",\n",
    "    \"zip\",\n",
    "    \"job\",\n",
    "    \"dob\",\n",
    "    \"trans_num\",\n",
    "    \"unix_time\",\n",
    "    \"Unnamed: 23\",\n",
    "    \"6006\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17112664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=drop_these, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3604cf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'amt', 'gender', 'state', 'lat', 'long', 'city_pop',\n",
       "       'merch_lat', 'merch_long', 'is_fraud'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee99efe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a12bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   category    1048575 non-null  object \n",
      " 1   amt         1048575 non-null  float64\n",
      " 2   gender      1048575 non-null  object \n",
      " 3   state       1048575 non-null  object \n",
      " 4   lat         1048575 non-null  float64\n",
      " 5   long        1048575 non-null  float64\n",
      " 6   city_pop    1048575 non-null  int64  \n",
      " 7   merch_lat   1048575 non-null  float64\n",
      " 8   merch_long  1048575 non-null  float64\n",
      " 9   is_fraud    1048575 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(3)\n",
      "memory usage: 80.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3899cfb",
   "metadata": {},
   "source": [
    "### Create Dictionaries for Encoding/Mapping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b68946",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df[\"category\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833836be",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d827b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entertainment',\n",
       " 'food_dining',\n",
       " 'gas_transport',\n",
       " 'grocery_net',\n",
       " 'grocery_pos',\n",
       " 'health_fitness',\n",
       " 'home',\n",
       " 'kids_pets',\n",
       " 'misc_net',\n",
       " 'misc_pos',\n",
       " 'personal_care',\n",
       " 'shopping_net',\n",
       " 'shopping_pos',\n",
       " 'travel']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3cba6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict = {category: float(idx) for idx, category in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d334bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entertainment': 0.0,\n",
       " 'food_dining': 1.0,\n",
       " 'gas_transport': 2.0,\n",
       " 'grocery_net': 3.0,\n",
       " 'grocery_pos': 4.0,\n",
       " 'health_fitness': 5.0,\n",
       " 'home': 6.0,\n",
       " 'kids_pets': 7.0,\n",
       " 'misc_net': 8.0,\n",
       " 'misc_pos': 9.0,\n",
       " 'personal_care': 10.0,\n",
       " 'shopping_net': 11.0,\n",
       " 'shopping_pos': 12.0,\n",
       " 'travel': 13.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e029de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = sorted(df[\"gender\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ae167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {gender: float(idx) for idx, gender in enumerate(genders)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85d7a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 0.0, 'M': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bead0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sorted(df[\"state\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ac4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "902f74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = {state: float(idx) for idx, state in enumerate(states)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddba3204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AK': 0.0,\n",
       " 'AL': 1.0,\n",
       " 'AR': 2.0,\n",
       " 'AZ': 3.0,\n",
       " 'CA': 4.0,\n",
       " 'CO': 5.0,\n",
       " 'CT': 6.0,\n",
       " 'DC': 7.0,\n",
       " 'DE': 8.0,\n",
       " 'FL': 9.0,\n",
       " 'GA': 10.0,\n",
       " 'HI': 11.0,\n",
       " 'IA': 12.0,\n",
       " 'ID': 13.0,\n",
       " 'IL': 14.0,\n",
       " 'IN': 15.0,\n",
       " 'KS': 16.0,\n",
       " 'KY': 17.0,\n",
       " 'LA': 18.0,\n",
       " 'MA': 19.0,\n",
       " 'MD': 20.0,\n",
       " 'ME': 21.0,\n",
       " 'MI': 22.0,\n",
       " 'MN': 23.0,\n",
       " 'MO': 24.0,\n",
       " 'MS': 25.0,\n",
       " 'MT': 26.0,\n",
       " 'NC': 27.0,\n",
       " 'ND': 28.0,\n",
       " 'NE': 29.0,\n",
       " 'NH': 30.0,\n",
       " 'NJ': 31.0,\n",
       " 'NM': 32.0,\n",
       " 'NV': 33.0,\n",
       " 'NY': 34.0,\n",
       " 'OH': 35.0,\n",
       " 'OK': 36.0,\n",
       " 'OR': 37.0,\n",
       " 'PA': 38.0,\n",
       " 'RI': 39.0,\n",
       " 'SC': 40.0,\n",
       " 'SD': 41.0,\n",
       " 'TN': 42.0,\n",
       " 'TX': 43.0,\n",
       " 'UT': 44.0,\n",
       " 'VA': 45.0,\n",
       " 'VT': 46.0,\n",
       " 'WA': 47.0,\n",
       " 'WI': 48.0,\n",
       " 'WV': 49.0,\n",
       " 'WY': 50.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e33f975",
   "metadata": {},
   "source": [
    "### Apply Encoding/Mapping to the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93f0aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"category\"].map(categories_dict)\n",
    "df[\"gender\"] = df[\"gender\"].map(gender_dict)\n",
    "df[\"state\"] = df[\"state\"].map(states_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d71986",
   "metadata": {},
   "source": [
    "### Convert all columns into Specific datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80c7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3be22",
   "metadata": {},
   "source": [
    "### Print details of the Final Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afe78d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   category    1048575 non-null  float32\n",
      " 1   amt         1048575 non-null  float32\n",
      " 2   gender      1048575 non-null  float32\n",
      " 3   state       1048575 non-null  float32\n",
      " 4   lat         1048575 non-null  float32\n",
      " 5   long        1048575 non-null  float32\n",
      " 6   city_pop    1048575 non-null  float32\n",
      " 7   merch_lat   1048575 non-null  float32\n",
      " 8   merch_long  1048575 non-null  float32\n",
      " 9   is_fraud    1048575 non-null  float32\n",
      "dtypes: float32(10)\n",
      "memory usage: 40.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2084e40",
   "metadata": {},
   "source": [
    "### End of Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d158a",
   "metadata": {},
   "source": [
    "# SAVING CLEANED DATA TO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8ad1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc593cc",
   "metadata": {},
   "source": [
    "# Read and Test datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f8bcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = pd.read_csv(\n",
    "    DATA_PATH, dtype=\"float32\"\n",
    ")  # Does not convert to float32 by default, dtype has to be explicitly provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# dq=dq.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d96ca42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   category    1048575 non-null  float32\n",
      " 1   amt         1048575 non-null  float32\n",
      " 2   gender      1048575 non-null  float32\n",
      " 3   state       1048575 non-null  float32\n",
      " 4   lat         1048575 non-null  float32\n",
      " 5   long        1048575 non-null  float32\n",
      " 6   city_pop    1048575 non-null  float32\n",
      " 7   merch_lat   1048575 non-null  float32\n",
      " 8   merch_long  1048575 non-null  float32\n",
      " 9   is_fraud    1048575 non-null  float32\n",
      "dtypes: float32(10)\n",
      "memory usage: 40.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dq.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997b2c7",
   "metadata": {},
   "source": [
    "# Creating DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41323816",
   "metadata": {},
   "source": [
    "### Clean Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "df4e6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(\n",
    "    df: pd.DataFrame, logger: Logger, extra_dropped_columns: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Cleans the input DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to be cleaned.\n",
    "        logger (Logger): Logger object for logging information.\n",
    "        extra_dropped_columns (List[str], optional): Columns to drop from the features in original dataset.\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    show_dataframe_info = True  # Set to True to log DataFrame info\n",
    "\n",
    "    # Log the initial state of the DataFrame\n",
    "    logger.info(f\"Initial DataFrame shape: {df.shape}\")\n",
    "\n",
    "    if show_dataframe_info:\n",
    "        buffer = io.StringIO()  # Create a buffer to capture the info output\n",
    "        df.info(buf=buffer)  # Store the output into the buffer\n",
    "        logger.info(f\"Initial DataFrame info:\\n \" + buffer.getvalue())\n",
    "\n",
    "    # Drop any unused columns\n",
    "    try:\n",
    "        df.drop(columns=extra_dropped_columns, inplace=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Problem dropping columns:\\n{e}\")\n",
    "\n",
    "    # Create dictionaries for mapping/encoding\n",
    "    categories = sorted(df[\"category\"].unique().tolist())\n",
    "    categories_dict = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "    genders = sorted(df[\"gender\"].unique().tolist())\n",
    "    gender_dict = {gender: idx for idx, gender in enumerate(genders)}\n",
    "\n",
    "    states = sorted(df[\"state\"].unique().tolist())\n",
    "    states_dict = {state: idx for idx, state in enumerate(states)}\n",
    "\n",
    "    logger.info(\"Encoding categorical variables...\")\n",
    "    try:\n",
    "        df[\"category\"] = df[\"category\"].map(categories_dict)\n",
    "        df[\"gender\"] = df[\"gender\"].map(gender_dict)\n",
    "        df[\"state\"] = df[\"state\"].map(states_dict)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Problem encoding columns, {e}\")\n",
    "\n",
    "    # Handle missing values (if any)\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        logger.info(\"Handling missing values...\")\n",
    "        df = df.dropna()  # Example: Drop rows with missing values\n",
    "        logger.info(f\"DataFrame shape after dropping missing values: {df.shape}\")\n",
    "\n",
    "    # Convert to 'float32' to reduce memory usage\n",
    "    logger.info(\"Converting Entire Data Frame to 'float32'...\")\n",
    "    df = df.astype(\"float32\")\n",
    "\n",
    "    if show_dataframe_info:\n",
    "        # Reinitialize the buffer to clear any previous content in order to log the final dataframe info\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        logger.info(f\"Final DataFrame info:\\n \" + buffer.getvalue())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c86fb",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0cd3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Dataset class For the Custom Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file: str = \"../Data/DataSplits/test.csv\", label_column: str = \"Label\"):\n",
    "        \"\"\"Initializer for the Dataset class.\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file containing the dataset.\n",
    "            label_column (str): The name of the column indicating the label.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)  # Assign a pandas data frame\n",
    "        except FileNotFoundError:  # Raise an error if the file is not found\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Define feature and label columns\n",
    "        self.label_column = label_column\n",
    "        # Omit the label column to create the list of feature columns\n",
    "        self.feature_columns = self.data.columns.drop([self.label_column])\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns a tuple (features, label) for the given index.\n",
    "        Args:\n",
    "            index (int): Index of the data sample to retrieve.\n",
    "        Returns:\n",
    "            tuple: (features, label) where features is a tensor of input features and label is the corresponding label.\n",
    "        \"\"\"\n",
    "        # Use 'iloc' instead of 'loc' for efficiency\n",
    "        features = self.data.iloc[index][self.feature_columns].values\n",
    "        label = self.data.iloc[index][self.label_column]  # Extract the label for the given index\n",
    "        return (torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the amount of samples in the dataset.\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c267da",
   "metadata": {},
   "source": [
    "### Data Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "df05a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(\n",
    "    logger: Logger,\n",
    "    dataset_url: str,\n",
    "    root_data_dir: str = \"../Data\",\n",
    "    data_file_path: str = \"Dataset.csv\",\n",
    "    data_splits_dir: str = \"DataSplits\",\n",
    "    scaler_dir=\"Scalers\",\n",
    "    target_column: str = \"Target\",\n",
    "    extra_dropped_columns: Optional[List[str]] = None,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = False,\n",
    "    drop_last: bool = True,\n",
    ") -> tuple[\n",
    "    Dataset, Dataset, Dataset, DataLoader, DataLoader, DataLoader, MinMaxScaler, MinMaxScaler\n",
    "]:\n",
    "    \"\"\"This function prepares the train, test, and validation datasets.\n",
    "    Args:\n",
    "        logger (Logger): The logger instance to log messages.\n",
    "        dataset_url (str): The URL to download the dataset from, if not found locally.\n",
    "        root_data_dir (str): The root of the Data Directory\n",
    "        data_file_path (str): The name of the original dataset (with .csv file extension).\n",
    "        data_splits_dir (str): Path to the train, test, and validation datasets.\n",
    "        scaler_dir (str): Path to the feature and label scalers.\n",
    "        target_column (str): The name of the target column to predict.\n",
    "        extra_dropped_columns (List[str], optional): Columns to drop from the features in original dataset.\n",
    "        batch_size (int): The dataloader's batch_size.\n",
    "        num_workers (int): The dataloader's number of workers.\n",
    "        pin_memory (bool): The dataloader's pin memory option.\n",
    "        drop_last (bool): The dataloader's drop_last option.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (Dataset): Dataset Class for the training dataset.\n",
    "        test_dataset (Dataset): Dataset Class for the test dataset.\n",
    "        validation_dataset (Dataset): Dataset Class for the validation dataset.\n",
    "        train_dataloader (DataLoader): The train dataloader.\n",
    "        test_dataloader (DataLoader): The test dataloader.\n",
    "        validation_dataloader (DataLoader): The validation dataloader.\n",
    "        feature_scaler (MinMaxScaler): The scaler used to scale the features of the model input.\n",
    "        label_scaler (MinMaxScaler): The scaler used to scale the labels of the model input.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        not root_data_dir or not data_file_path or not data_splits_dir\n",
    "    ):  # Check for empty strings at the beginning\n",
    "        raise ValueError(\"File and directory paths cannot be empty strings.\")\n",
    "    DATA_ROOT = Path(root_data_dir)\n",
    "\n",
    "    DATA_CLEAN_PATH = DATA_ROOT / data_file_path  # Set the path to the complete dataset\n",
    "\n",
    "    if DATA_CLEAN_PATH.exists():\n",
    "        logger.info(f\"CSV file detected, reading from '{DATA_ROOT}'\")\n",
    "        df = pd.read_csv(\n",
    "            DATA_CLEAN_PATH, dtype=\"float32\"\n",
    "        )  # Convert data to float32 instead of, float64\n",
    "    else:\n",
    "        logger.info(f\"Downloading CSV file from '{dataset_url}'\\nand saving into '{DATA_ROOT}'\")\n",
    "        try:\n",
    "            os.makedirs(DATA_ROOT, exist_ok=True)  # Create the Data Root Directory\n",
    "            # Download and read the data into a pandas dataframe\n",
    "            df = pd.read_csv(dataset_url)  # Keep data as is, may not be able to expect float32 data\n",
    "\n",
    "            # Clean the data before saving\n",
    "            try:\n",
    "                df = clean_data(df, logger, extra_dropped_columns=extra_dropped_columns)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"An unexpected error occurred cleaning the dataset:\\n{e}\")\n",
    "\n",
    "            df.to_csv(DATA_CLEAN_PATH, index=False)  # Save the file, omitting saving the row index\n",
    "        except OSError as e:\n",
    "            raise RuntimeError(f\"OS error occurred: {e}\")\n",
    "        except ParserError:\n",
    "            raise RuntimeError(f\"Failed to parse CSV from '{dataset_url}'\")\n",
    "        except ValueError as e:\n",
    "            raise RuntimeError(f\"Data cleaning error:\\n{e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"An unexpected error occurred when downloading or saving the \"\n",
    "                f\"dataset from '{dataset_url}' to '{DATA_CLEAN_PATH}':\\n{e}\"\n",
    "            )\n",
    "\n",
    "    # Define the paths for the data splits and scalers\n",
    "    DATA_SPLITS_DIR = DATA_ROOT / data_splits_dir\n",
    "    SCALER_DIR = DATA_ROOT / scaler_dir\n",
    "\n",
    "    TRAIN_DATA_PATH = DATA_SPLITS_DIR / \"train.csv\"\n",
    "    TEST_DATA_PATH = DATA_SPLITS_DIR / \"test.csv\"\n",
    "    VALIDATION_DATA_PATH = DATA_SPLITS_DIR / \"val.csv\"\n",
    "\n",
    "    FEATURE_SCALER_PATH = SCALER_DIR / \"feature-scaler.joblib\"\n",
    "    LABEL_SCALER_PATH = SCALER_DIR / \"label-scaler.joblib\"\n",
    "\n",
    "    # Dictate whether to use label scaler\n",
    "    USE_LABEL_SCALER = False\n",
    "\n",
    "    # Define the columns to drop from the features\n",
    "    columns_to_drop = [target_column]\n",
    "\n",
    "    # Define the Data Splits\n",
    "    TRAIN_SPLIT_PERCENTAGE = 0.9\n",
    "    VALIDATION_SPLIT_PERCENTAGE = 0.5\n",
    "\n",
    "    if (\n",
    "        os.path.exists(TRAIN_DATA_PATH)\n",
    "        and os.path.exists(TEST_DATA_PATH)\n",
    "        and os.path.exists(VALIDATION_DATA_PATH)\n",
    "    ):\n",
    "        logger.info(\n",
    "            f\"Train, Test, and Validation CSV datasets detected in '{DATA_SPLITS_DIR}.' Skipping generation and loading scaler(s)\"\n",
    "        )\n",
    "        try:\n",
    "            feature_scaler = joblib.load(FEATURE_SCALER_PATH)\n",
    "            logger.info(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            if USE_LABEL_SCALER:\n",
    "                joblib.dump(\n",
    "                    label_scaler, LABEL_SCALER_PATH\n",
    "                )  # Not used for this classification task\n",
    "                logger.info(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "            else:\n",
    "                label_scaler = None  # Omit the label scaler loading\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Scaler file not found: {e}\")\n",
    "        except EOFError as e:\n",
    "            raise RuntimeError(f\"Scaler file appears to be empty or corrupted: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when loading scalers: {e}\")\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Datasets not found in '{DATA_SPLITS_DIR}' or incomplete. Generating datasets...\"\n",
    "        )\n",
    "        os.makedirs(DATA_SPLITS_DIR, exist_ok=True)  # Create the Data Splits Parent Directory\n",
    "        os.makedirs(SCALER_DIR, exist_ok=True)  # Create the Scaler Parent Directory\n",
    "\n",
    "        # Create the scaler objects\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        if USE_LABEL_SCALER:\n",
    "            label_scaler = MinMaxScaler()\n",
    "        else:\n",
    "            label_scaler = None  # Not used for this Classification task\n",
    "\n",
    "        try:\n",
    "            df_features = df.drop(columns=columns_to_drop, inplace=False)\n",
    "            df_labels = df[\n",
    "                [target_column]\n",
    "            ]  # Instead of returning a pandas Series using \"[]\", return a dataframe using the \"[[]]\" to get a shape with (-1,1)\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"One or more specified columns to drop do not exist in the DataFrame: {e}\"\n",
    "            )\n",
    "\n",
    "        # Use OverSampling Technique to Balance out the Dataset for this Unbalanced Dataset\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        df_features_resampled, df_labels_resampled = ros.fit_resample(df_features, df_labels)\n",
    "\n",
    "        # Split into smaller DataFrames for the Train, Test, and Validation splits\n",
    "        X_train, X_inter, Y_train, Y_inter = train_test_split(\n",
    "            df_features_resampled,\n",
    "            df_labels_resampled,\n",
    "            test_size=1 - TRAIN_SPLIT_PERCENTAGE,\n",
    "            random_state=42,\n",
    "        )\n",
    "        X_validation, X_test, Y_validation, Y_test = train_test_split(\n",
    "            X_inter, Y_inter, test_size=1 - VALIDATION_SPLIT_PERCENTAGE, random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit the scalers to the data\n",
    "        feature_scaler.fit(X_train)\n",
    "        # Only scale the labels if required\n",
    "        if USE_LABEL_SCALER:\n",
    "            label_scaler.fit(Y_train)  # Not used for this Classification task\n",
    "\n",
    "        # Save the fitted scaler object\n",
    "        try:\n",
    "            joblib.dump(feature_scaler, FEATURE_SCALER_PATH)\n",
    "            logger.info(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            # Save the Label Scaler if utilized\n",
    "            if USE_LABEL_SCALER:\n",
    "                joblib.dump(\n",
    "                    label_scaler, LABEL_SCALER_PATH\n",
    "                )  # Not used for this Classification task\n",
    "                logger.info(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Save path not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when saving  Scaler(s): {e}\")\n",
    "\n",
    "        # Scale all Feature Inputs\n",
    "        X_train_scaled = feature_scaler.transform(X_train)\n",
    "        X_validation_scaled = feature_scaler.transform(X_validation)\n",
    "        X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "        if USE_LABEL_SCALER:  # Not used for this Classification task\n",
    "            Y_train = label_scaler.transform(Y_train)\n",
    "            Y_validation = label_scaler.transform(Y_validation)\n",
    "            Y_test = label_scaler.transform(Y_test)\n",
    "\n",
    "        logger.info(f\"Train Features (Scaled) Shape: {X_train_scaled.shape}\")\n",
    "        logger.info(f\"Validation Features (Scaled) Shape: {X_validation_scaled.shape}\")\n",
    "        logger.info(f\"Test Features (Scaled) Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        if USE_LABEL_SCALER:\n",
    "            logger.info(f\"Train Labels (Scaled) Shape: {Y_train.shape}\")\n",
    "            logger.info(f\"Validation Labels (Scaled) Shape: {Y_validation.shape}\")\n",
    "            logger.info(f\"Test Labels (Scaled) Shape: {Y_test.shape}\")\n",
    "        else:\n",
    "            logger.info(f\"Train Labels Shape: {Y_train.shape}\")\n",
    "            logger.info(f\"Validation Labels Shape: {Y_validation.shape}\")\n",
    "            logger.info(f\"Test Labels Shape: {Y_test.shape}\")\n",
    "\n",
    "        # Define the column names of the features and label\n",
    "        features_names = df_features.columns\n",
    "        label_name = df_labels.columns\n",
    "\n",
    "        # Create dataframes using the scaled data\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=features_names)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=features_names)\n",
    "        X_validation_df = pd.DataFrame(X_validation_scaled, columns=features_names)\n",
    "        Y_train_df = pd.DataFrame(Y_train, columns=label_name)\n",
    "        Y_test_df = pd.DataFrame(Y_test, columns=label_name)\n",
    "        Y_validation_df = pd.DataFrame(Y_validation, columns=label_name)\n",
    "\n",
    "        # Concatenate the features and labels back into a single DataFrame for each set\n",
    "        train_data_frame = pd.concat([X_train_df, Y_train_df.reset_index(drop=True)], axis=1)\n",
    "        test_data_frame = pd.concat([X_test_df, Y_test_df.reset_index(drop=True)], axis=1)\n",
    "        validation_data_frame = pd.concat(\n",
    "            [X_validation_df, Y_validation_df.reset_index(drop=True)], axis=1\n",
    "        )\n",
    "\n",
    "        # Saving the split data to csv files\n",
    "        try:\n",
    "            train_data_frame.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "            test_data_frame.to_csv(TEST_DATA_PATH, index=False)\n",
    "            validation_data_frame.to_csv(VALIDATION_DATA_PATH, index=False)\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(f\"Save path not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"An unexpected error occurred when saving datasets to CSV files:\\n{e}\"\n",
    "            )\n",
    "\n",
    "    # Creating Datasets from the stored datasets\n",
    "    logger.info(f\"INITIALIZING DATASETS\")\n",
    "    train_dataset = CustomDataset(csv_file=TRAIN_DATA_PATH, label_column=target_column)\n",
    "    test_dataset = CustomDataset(csv_file=TEST_DATA_PATH, label_column=target_column)\n",
    "    val_dataset = CustomDataset(csv_file=VALIDATION_DATA_PATH, label_column=target_column)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Creating DataLoaders with 'batch_size'=({batch_size}), 'num_workers'=({num_workers}), 'pin_memory'=({pin_memory}). Training dataset 'drop_last'=({drop_last})\"\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Training DataLoader has ({len(train_dataloader)}) batches, Test DataLoader has ({len(test_dataloader)}) batches, Validation DataLoader has ({len(validation_dataloader)}) batches\"\n",
    "    )\n",
    "\n",
    "    logger.info(\"==================================================================\")\n",
    "    for name, dataloader in [\n",
    "        (\"Train\", train_dataloader),\n",
    "        (\"Validation\", validation_dataloader),\n",
    "        (\"Test\", test_dataloader),\n",
    "    ]:\n",
    "        features, labels = next(iter(dataloader))  # Get one batch\n",
    "\n",
    "        logger.info(f\"{name} Dataloader Batch Information\")\n",
    "        logger.info(f\"Features Shape: '{features.shape}' |  DataTypes: '{features.dtype}'\")\n",
    "        logger.info(f\"Labels Shape: '{labels.shape}'   |  DataTypes: '{labels.dtype}' \")\n",
    "        logger.info(\"==================================================================\")\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        val_dataset,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        feature_scaler,\n",
    "        label_scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6b97a",
   "metadata": {},
   "source": [
    "# Testing the Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c166aa6",
   "metadata": {},
   "source": [
    "## Testing with the raw dataset as a fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2f1a6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED WHEN TESTING THE RAW DATASET\n",
    "def test_data_pipeline():\n",
    "    # Function input setup\n",
    "    data = {\n",
    "        \"dataset_url\": \"hf://datasets/dazzle-nu/CIS435-CreditCardFraudDetection/fraudTrain.csv\",\n",
    "        \"root_data_dir\": \"../Data\",\n",
    "        \"data_file_path\": DATA_CLEAN_FILE_NAME,\n",
    "        \"data_splits_dir\": \"DataSplits\",\n",
    "        \"scaler_dir\": \"Scalers\",\n",
    "        \"target_column\": \"is_fraud\",\n",
    "        \"extra_dropped_columns\": [\n",
    "            \"Unnamed: 0\",\n",
    "            \"trans_date_trans_time\",\n",
    "            \"cc_num\",\n",
    "            \"merchant\",\n",
    "            \"first\",\n",
    "            \"last\",\n",
    "            \"street\",\n",
    "            \"city\",\n",
    "            \"zip\",\n",
    "            \"job\",\n",
    "            \"dob\",\n",
    "            \"trans_num\",\n",
    "            \"unix_time\",\n",
    "            \"Unnamed: 23\",\n",
    "            \"6006\",\n",
    "        ],\n",
    "    }\n",
    "    batch_size = 64\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    drop_last = True\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Call the data pipeline function\n",
    "    try:\n",
    "        (\n",
    "            train_dataset,\n",
    "            test_dataset,\n",
    "            val_dataset,\n",
    "            train_dataloader,\n",
    "            test_dataloader,\n",
    "            validation_dataloader,\n",
    "            feature_scaler,\n",
    "            label_scaler,\n",
    "        ) = data_pipeline(\n",
    "            logger,\n",
    "            **data,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Caught Exception: {e}\", stack_info=True)\n",
    "\n",
    "    # Basic assertions to verify the outputs\n",
    "    assert isinstance(train_dataset, Dataset), \"train_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(test_dataset, Dataset), \"test_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(val_dataset, Dataset), \"val_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(\n",
    "        train_dataloader, DataLoader\n",
    "    ), \"train_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        test_dataloader, DataLoader\n",
    "    ), \"test_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        validation_dataloader, DataLoader\n",
    "    ), \"validation_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        feature_scaler, MinMaxScaler\n",
    "    ), \"feature_scaler is not an instance of MinMaxScaler\"\n",
    "    # assert isinstance(label_scaler, MinMaxScaler), \"label_scaler is not an instance of MinMaxScaler\"\n",
    "\n",
    "    logger.info(\"All assertions passed. Data pipeline test successful.\")\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        val_dataset,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        feature_scaler,\n",
    "        label_scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "09a32677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloading CSV file from 'hf://datasets/dazzle-nu/CIS435-CreditCardFraudDetection/fraudTrain.csv'\n",
      "and saving into '..\\Data'\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/dazzle-nu/CIS435-CreditCardFraudDetection/resolve/main/fraudTrain.csv\n",
      "WARNING:huggingface_hub.utils._http:'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/dazzle-nu/CIS435-CreditCardFraudDetection/resolve/main/fraudTrain.csv\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "INFO:__main__:Initial DataFrame shape: (1048575, 25)\n",
      "INFO:__main__:Initial DataFrame info:\n",
      " <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   Unnamed: 0             1048575 non-null  int64  \n",
      " 1   trans_date_trans_time  1048575 non-null  object \n",
      " 2   cc_num                 1048575 non-null  float64\n",
      " 3   merchant               1048575 non-null  object \n",
      " 4   category               1048575 non-null  object \n",
      " 5   amt                    1048575 non-null  float64\n",
      " 6   first                  1048575 non-null  object \n",
      " 7   last                   1048575 non-null  object \n",
      " 8   gender                 1048575 non-null  object \n",
      " 9   street                 1048575 non-null  object \n",
      " 10  city                   1048575 non-null  object \n",
      " 11  state                  1048575 non-null  object \n",
      " 12  zip                    1048575 non-null  int64  \n",
      " 13  lat                    1048575 non-null  float64\n",
      " 14  long                   1048575 non-null  float64\n",
      " 15  city_pop               1048575 non-null  int64  \n",
      " 16  job                    1048575 non-null  object \n",
      " 17  dob                    1048575 non-null  object \n",
      " 18  trans_num              1048575 non-null  object \n",
      " 19  unix_time              1048575 non-null  int64  \n",
      " 20  merch_lat              1048575 non-null  float64\n",
      " 21  merch_long             1048575 non-null  float64\n",
      " 22  is_fraud               1048575 non-null  int64  \n",
      " 23  Unnamed: 23            0 non-null        float64\n",
      " 24  6006                   0 non-null        float64\n",
      "dtypes: float64(8), int64(5), object(12)\n",
      "memory usage: 200.0+ MB\n",
      "\n",
      "INFO:__main__:Encoding categorical variables...\n",
      "INFO:__main__:Converting Entire Data Frame to 'float32'...\n",
      "INFO:__main__:Final DataFrame info:\n",
      " <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   category    1048575 non-null  float32\n",
      " 1   amt         1048575 non-null  float32\n",
      " 2   gender      1048575 non-null  float32\n",
      " 3   state       1048575 non-null  float32\n",
      " 4   lat         1048575 non-null  float32\n",
      " 5   long        1048575 non-null  float32\n",
      " 6   city_pop    1048575 non-null  float32\n",
      " 7   merch_lat   1048575 non-null  float32\n",
      " 8   merch_long  1048575 non-null  float32\n",
      " 9   is_fraud    1048575 non-null  float32\n",
      "dtypes: float32(10)\n",
      "memory usage: 40.0 MB\n",
      "\n",
      "INFO:__main__:Datasets not found in '..\\Data\\DataSplits' or incomplete. Generating datasets...\n",
      "INFO:__main__:Feature scaler stored in: (..\\Data\\Scalers\\feature-scaler.joblib)\n",
      "INFO:__main__:Train Features (Scaled) Shape: (1876624, 9)\n",
      "INFO:__main__:Validation Features (Scaled) Shape: (104257, 9)\n",
      "INFO:__main__:Test Features (Scaled) Shape: (104257, 9)\n",
      "INFO:__main__:Train Labels Shape: (1876624, 1)\n",
      "INFO:__main__:Validation Labels Shape: (104257, 1)\n",
      "INFO:__main__:Test Labels Shape: (104257, 1)\n",
      "INFO:__main__:INITIALIZING DATASETS\n",
      "INFO:__main__:Creating DataLoaders with 'batch_size'=(64), 'num_workers'=(0), 'pin_memory'=(False). Training dataset 'drop_last'=(True)\n",
      "INFO:__main__:Training DataLoader has (29322) batches, Test DataLoader has (1629) batches, Validation DataLoader has (1629) batches\n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Train Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Validation Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Test Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:All assertions passed. Data pipeline test successful.\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    val_dataset,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    validation_dataloader,\n",
    "    feature_scaler,\n",
    "    label_scaler,\n",
    ") = test_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "17c5eda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Train Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:The labels: tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Validation Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:The labels: tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1])\n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Test Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:The labels: tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0])\n",
      "INFO:__main__:==================================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"==================================================================\")\n",
    "for name, dataloader in [\n",
    "    (\"Train\", train_dataloader),\n",
    "    (\"Validation\", validation_dataloader),\n",
    "    (\"Test\", test_dataloader),\n",
    "]:\n",
    "    features, labels = next(iter(dataloader))  # Get one batch\n",
    "\n",
    "    logger.info(f\"{name} Dataloader Batch Information\")\n",
    "    logger.info(f\"Features Shape: '{features.shape}' |  DataTypes: '{features.dtype}'\")\n",
    "    logger.info(f\"Labels Shape: '{labels.shape}'   |  DataTypes: '{labels.dtype}' \")\n",
    "    logger.info(f\"The labels: {labels}\")  # Optional\n",
    "    logger.info(\"==================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79056db",
   "metadata": {},
   "source": [
    "## Testing with the cleaned dataset as a fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bfb6107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED WHEN TESTING THE CLEAN DATASET\n",
    "def test_data_pipeline_2():\n",
    "    # Function input setup\n",
    "    data = {\n",
    "        \"dataset_url\": \"hf://datasets/MaxPrestige/credit-card-fraud-CLEAN/credit-card-fraud-CLEAN.csv\",\n",
    "        \"root_data_dir\": \"../Data\",\n",
    "        \"data_file_path\": DATA_CLEAN_FILE_NAME,\n",
    "        \"data_splits_dir\": \"DataSplits\",\n",
    "        \"scaler_dir\": \"Scalers\",\n",
    "        \"target_column\": \"is_fraud\",\n",
    "        \"extra_dropped_columns\": [],\n",
    "    }\n",
    "    batch_size = 64\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    drop_last = True\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Call the data pipeline function\n",
    "    try:\n",
    "        (\n",
    "            train_dataset,\n",
    "            test_dataset,\n",
    "            val_dataset,\n",
    "            train_dataloader,\n",
    "            test_dataloader,\n",
    "            validation_dataloader,\n",
    "            feature_scaler,\n",
    "            label_scaler,\n",
    "        ) = data_pipeline(\n",
    "            logger,\n",
    "            **data,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Caught Exception: {e}\", stack_info=True)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Basic assertions to verify the outputs\n",
    "    assert isinstance(train_dataset, Dataset), \"train_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(test_dataset, Dataset), \"test_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(val_dataset, Dataset), \"val_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(\n",
    "        train_dataloader, DataLoader\n",
    "    ), \"train_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        test_dataloader, DataLoader\n",
    "    ), \"test_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        validation_dataloader, DataLoader\n",
    "    ), \"validation_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(\n",
    "        feature_scaler, MinMaxScaler\n",
    "    ), \"feature_scaler is not an instance of MinMaxScaler\"\n",
    "    # assert isinstance(label_scaler, MinMaxScaler), \"label_scaler is not an instance of MinMaxScaler\" # Label Scaler Not Utilized\n",
    "\n",
    "    logger.info(\"All assertions passed. Data pipeline test successful.\")\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        val_dataset,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        validation_dataloader,\n",
    "        feature_scaler,\n",
    "        label_scaler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1af85d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CSV file detected, reading from '..\\Data'\n",
      "INFO:__main__:Train, Test, and Validation CSV datasets detected in '..\\Data\\DataSplits.' Skipping generation and loading scaler(s)\n",
      "INFO:__main__:Feature scaler stored in: (..\\Data\\Scalers\\feature-scaler.joblib)\n",
      "INFO:__main__:INITIALIZING DATASETS\n",
      "INFO:__main__:Creating DataLoaders with 'batch_size'=(64), 'num_workers'=(0), 'pin_memory'=(False). Training dataset 'drop_last'=(True)\n",
      "INFO:__main__:Training DataLoader has (29322) batches, Test DataLoader has (1629) batches, Validation DataLoader has (1629) batches\n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Train Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Validation Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Test Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:All assertions passed. Data pipeline test successful.\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    val_dataset,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    validation_dataloader,\n",
    "    feature_scaler,\n",
    "    label_scaler,\n",
    ") = test_data_pipeline_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ddf21ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75c6c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Train Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:The labels: tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0])\n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Validation Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:The labels: tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1])\n",
      "INFO:__main__:==================================================================\n",
      "INFO:__main__:Test Dataloader Batch Information\n",
      "INFO:__main__:Features Shape: 'torch.Size([64, 9])' |  DataTypes: 'torch.float32'\n",
      "INFO:__main__:Labels Shape: 'torch.Size([64])'   |  DataTypes: 'torch.int64' \n",
      "INFO:__main__:The labels: tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0])\n",
      "INFO:__main__:==================================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"==================================================================\")\n",
    "for name, dataloader in [\n",
    "    (\"Train\", train_dataloader),\n",
    "    (\"Validation\", validation_dataloader),\n",
    "    (\"Test\", test_dataloader),\n",
    "]:\n",
    "    features, labels = next(iter(dataloader))  # Get one batch\n",
    "\n",
    "    logger.info(f\"{name} Dataloader Batch Information\")\n",
    "    logger.info(f\"Features Shape: '{features.shape}' |  DataTypes: '{features.dtype}'\")\n",
    "    logger.info(f\"Labels Shape: '{labels.shape}'   |  DataTypes: '{labels.dtype}' \")\n",
    "    logger.info(f\"The labels: {labels}\")  # Optional\n",
    "    logger.info(\"==================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
