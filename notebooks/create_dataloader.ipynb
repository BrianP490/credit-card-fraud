{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc715cf",
   "metadata": {},
   "source": [
    "# Cleaning Data and Preparing DataLoader\n",
    "\n",
    "- Look into using parquet files to shrink data files down (comes at the cost of not being human readable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad075ef",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34672e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os, logging, joblib, io\n",
    "from logging import Logger\n",
    "from typing import List, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "from importlib.metadata import version\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db165c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886c558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:pandas version: 2.3.2\n",
      "INFO:__main__:importlib-metadata version: 8.7.0\n",
      "INFO:__main__:pyarrow version: 21.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Could not get version for package imblearn: No package metadata was found for imblearn\n"
     ]
    }
   ],
   "source": [
    "packages = [\"pandas\", \"importlib-metadata\", \"pyarrow\"]\n",
    "for package in packages:\n",
    "    try:\n",
    "        logger.info(f\"{package} version: {version(package)}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not get version for package {package}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040322d",
   "metadata": {},
   "source": [
    "## Load Dataframe from csv file in local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c642d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../Data\")\n",
    "RAW_DATA_DIR_NAME = \"Data\"\n",
    "# DATA_RAW_FILE_NAME = \"credit-card-fraud.csv\"\n",
    "\n",
    "DATA_RAW_FILE_NAME = \"credit-card-fraud-RAW.csv\"\n",
    "DATA_CLEAN_FILE_NAME = \"credit-card-fraud-CLEAN.csv\"\n",
    "\n",
    "TEMP_DATA_PATH = DATA_ROOT / RAW_DATA_DIR_NAME / DATA_RAW_FILE_NAME\n",
    "DATA_PATH = DATA_ROOT / RAW_DATA_DIR_NAME / DATA_CLEAN_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2d47ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TEMP_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50211765",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631a6e8",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3845851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these =[\"Unnamed: 0\", \"trans_date_trans_time\", \"cc_num\", \"merchant\", \"first\", \"last\", \"street\", \"city\", \"zip\", \"job\", \"dob\", \"trans_num\", \"unix_time\", \"Unnamed: 23\", \"6006\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "17112664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=drop_these, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3604cf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'amt', 'gender', 'state', 'lat', 'long', 'city_pop',\n",
       "       'merch_lat', 'merch_long', 'is_fraud'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ee99efe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 10)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "34a12bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   category    1048575 non-null  object \n",
      " 1   amt         1048575 non-null  float64\n",
      " 2   gender      1048575 non-null  object \n",
      " 3   state       1048575 non-null  object \n",
      " 4   lat         1048575 non-null  float64\n",
      " 5   long        1048575 non-null  float64\n",
      " 6   city_pop    1048575 non-null  int64  \n",
      " 7   merch_lat   1048575 non-null  float64\n",
      " 8   merch_long  1048575 non-null  float64\n",
      " 9   is_fraud    1048575 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(3)\n",
      "memory usage: 80.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3899cfb",
   "metadata": {},
   "source": [
    "### Create Dictionaries for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "97b68946",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df[\"category\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "833836be",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "64d827b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entertainment',\n",
       " 'food_dining',\n",
       " 'gas_transport',\n",
       " 'grocery_net',\n",
       " 'grocery_pos',\n",
       " 'health_fitness',\n",
       " 'home',\n",
       " 'kids_pets',\n",
       " 'misc_net',\n",
       " 'misc_pos',\n",
       " 'personal_care',\n",
       " 'shopping_net',\n",
       " 'shopping_pos',\n",
       " 'travel']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f3cba6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict = {category: float(idx) for idx, category in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6d334bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entertainment': 0.0,\n",
       " 'food_dining': 1.0,\n",
       " 'gas_transport': 2.0,\n",
       " 'grocery_net': 3.0,\n",
       " 'grocery_pos': 4.0,\n",
       " 'health_fitness': 5.0,\n",
       " 'home': 6.0,\n",
       " 'kids_pets': 7.0,\n",
       " 'misc_net': 8.0,\n",
       " 'misc_pos': 9.0,\n",
       " 'personal_care': 10.0,\n",
       " 'shopping_net': 11.0,\n",
       " 'shopping_pos': 12.0,\n",
       " 'travel': 13.0}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = sorted(df[\"gender\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e2ae167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {gender: float(idx) for idx, gender in enumerate(genders)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a85d7a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 0.0, 'M': 1.0}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sorted(df[\"state\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a0ac4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "902f74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = {state: float(idx) for idx, state in enumerate(states)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ddba3204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AK': 0.0,\n",
       " 'AL': 1.0,\n",
       " 'AR': 2.0,\n",
       " 'AZ': 3.0,\n",
       " 'CA': 4.0,\n",
       " 'CO': 5.0,\n",
       " 'CT': 6.0,\n",
       " 'DC': 7.0,\n",
       " 'DE': 8.0,\n",
       " 'FL': 9.0,\n",
       " 'GA': 10.0,\n",
       " 'HI': 11.0,\n",
       " 'IA': 12.0,\n",
       " 'ID': 13.0,\n",
       " 'IL': 14.0,\n",
       " 'IN': 15.0,\n",
       " 'KS': 16.0,\n",
       " 'KY': 17.0,\n",
       " 'LA': 18.0,\n",
       " 'MA': 19.0,\n",
       " 'MD': 20.0,\n",
       " 'ME': 21.0,\n",
       " 'MI': 22.0,\n",
       " 'MN': 23.0,\n",
       " 'MO': 24.0,\n",
       " 'MS': 25.0,\n",
       " 'MT': 26.0,\n",
       " 'NC': 27.0,\n",
       " 'ND': 28.0,\n",
       " 'NE': 29.0,\n",
       " 'NH': 30.0,\n",
       " 'NJ': 31.0,\n",
       " 'NM': 32.0,\n",
       " 'NV': 33.0,\n",
       " 'NY': 34.0,\n",
       " 'OH': 35.0,\n",
       " 'OK': 36.0,\n",
       " 'OR': 37.0,\n",
       " 'PA': 38.0,\n",
       " 'RI': 39.0,\n",
       " 'SC': 40.0,\n",
       " 'SD': 41.0,\n",
       " 'TN': 42.0,\n",
       " 'TX': 43.0,\n",
       " 'UT': 44.0,\n",
       " 'VA': 45.0,\n",
       " 'VT': 46.0,\n",
       " 'WA': 47.0,\n",
       " 'WI': 48.0,\n",
       " 'WV': 49.0,\n",
       " 'WY': 50.0}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e33f975",
   "metadata": {},
   "source": [
    "### Apply mapping to the columns to convert to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "93f0aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"category\"].map(categories_dict)\n",
    "df[\"gender\"] = df[\"gender\"].map(gender_dict)\n",
    "df[\"state\"] = df[\"state\"].map(states_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d71986",
   "metadata": {},
   "source": [
    "### Convert all columns into float32 datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b80c7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942a60b",
   "metadata": {},
   "source": [
    "### End of Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "afe78d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   category    1048575 non-null  float32\n",
      " 1   amt         1048575 non-null  float32\n",
      " 2   gender      1048575 non-null  float32\n",
      " 3   state       1048575 non-null  float32\n",
      " 4   lat         1048575 non-null  float32\n",
      " 5   long        1048575 non-null  float32\n",
      " 6   city_pop    1048575 non-null  float32\n",
      " 7   merch_lat   1048575 non-null  float32\n",
      " 8   merch_long  1048575 non-null  float32\n",
      " 9   is_fraud    1048575 non-null  float32\n",
      "dtypes: float32(10)\n",
      "memory usage: 40.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d158a",
   "metadata": {},
   "source": [
    "# SAVING CLEANED DATA TO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a8ad1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different file extensions\n",
    "# df.to_feather(\"data.feather\")\n",
    "# df.to_parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc593cc",
   "metadata": {},
   "source": [
    "# read and test datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8bcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = pd.read_csv(DATA_PATH, dtype='float32') # Does not convert to float32 by default, dtype has to be explicitly provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# dq=dq.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d96ca42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   category    1048575 non-null  float32\n",
      " 1   amt         1048575 non-null  float32\n",
      " 2   gender      1048575 non-null  float32\n",
      " 3   state       1048575 non-null  float32\n",
      " 4   lat         1048575 non-null  float32\n",
      " 5   long        1048575 non-null  float32\n",
      " 6   city_pop    1048575 non-null  float32\n",
      " 7   merch_lat   1048575 non-null  float32\n",
      " 8   merch_long  1048575 non-null  float32\n",
      " 9   is_fraud    1048575 non-null  float32\n",
      "dtypes: float32(10)\n",
      "memory usage: 40.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dq.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db954ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.970000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.078800</td>\n",
       "      <td>-81.178101</td>\n",
       "      <td>3495.0</td>\n",
       "      <td>36.011292</td>\n",
       "      <td>-82.048317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>107.230003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.887798</td>\n",
       "      <td>-118.210503</td>\n",
       "      <td>149.0</td>\n",
       "      <td>49.159046</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>220.110001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>42.180801</td>\n",
       "      <td>-112.262001</td>\n",
       "      <td>4154.0</td>\n",
       "      <td>43.150703</td>\n",
       "      <td>-112.154480</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>46.230598</td>\n",
       "      <td>-112.113800</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>47.034332</td>\n",
       "      <td>-112.561073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>41.959999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>38.420700</td>\n",
       "      <td>-79.462898</td>\n",
       "      <td>99.0</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632462</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category         amt  gender  state        lat        long  city_pop  \\\n",
       "0       8.0    4.970000     0.0   27.0  36.078800  -81.178101    3495.0   \n",
       "1       4.0  107.230003     0.0   47.0  48.887798 -118.210503     149.0   \n",
       "2       0.0  220.110001     1.0   13.0  42.180801 -112.262001    4154.0   \n",
       "3       2.0   45.000000     1.0   26.0  46.230598 -112.113800    1939.0   \n",
       "4       9.0   41.959999     1.0   45.0  38.420700  -79.462898      99.0   \n",
       "\n",
       "   merch_lat  merch_long  is_fraud  \n",
       "0  36.011292  -82.048317       0.0  \n",
       "1  49.159046 -118.186462       0.0  \n",
       "2  43.150703 -112.154480       0.0  \n",
       "3  47.034332 -112.561073       0.0  \n",
       "4  38.674999  -78.632462       0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997b2c7",
   "metadata": {},
   "source": [
    "# Creating DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41323816",
   "metadata": {},
   "source": [
    "### Clean Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4e6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame, logger: Logger, dropped_columns: Optional[List[str]]=None) -> pd.DataFrame:\n",
    "    \"\"\"Cleans the input DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to be cleaned.\n",
    "        logger (Logger): Logger object for logging information.\n",
    "        dropped_columns (List[str], optional): Columns to drop from the features in original dataset.\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    show_dataframe_info = True  # Set to True to log DataFrame info\n",
    "\n",
    "    # Log the initial state of the DataFrame\n",
    "    logger.info(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    if show_dataframe_info:\n",
    "        buffer = io.StringIO()  # Create a buffer to capture the info output\n",
    "        df.info(buf=buffer) # Store the output into the buffer\n",
    "        logger.info(f\"Initial DataFrame info:\\n \" + buffer.getvalue())\n",
    "    \n",
    "    # Drop any unused columns\n",
    "    try:\n",
    "        df.drop(columns=dropped_columns, inplace=True)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Problem dropping columns, {e}\")\n",
    "\n",
    "\n",
    "    # Create dictionaries for mapping/encoding\n",
    "    categories = sorted(df[\"category\"].unique().tolist())\n",
    "    categories_dict = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "    genders = sorted(df[\"gender\"].unique().tolist())\n",
    "    gender_dict = {gender: idx for idx, gender in enumerate(genders)}\n",
    "\n",
    "    states = sorted(df[\"state\"].unique().tolist())\n",
    "    # Encode categorical variables and convert to float\n",
    "    states_dict = {state: idx for idx, state in enumerate(states)}\n",
    "\n",
    "\n",
    "    logger.info(\"Encoding categorical variables...\")\n",
    "    try:\n",
    "        df['category'] = df['category'].map(categories_dict)\n",
    "        df['gender'] = df['gender'].map(gender_dict)\n",
    "        df['state'] = df['state'].map(states_dict)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Problem encoding columns, {e}\")\n",
    "        \n",
    "    # Handle missing values (if any)\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        logger.info(\"Handling missing values...\")\n",
    "        df = df.dropna()  # Example: Drop rows with missing values\n",
    "        logger.info(f\"DataFrame shape after dropping missing values: {df.shape}\")\n",
    "    \n",
    "    # Convert to 'float32' to reduce memory usage\n",
    "    logger.info(\"Converting Data Frame to 'float32'...\")\n",
    "    df = df.astype('float32')\n",
    "\n",
    "    if show_dataframe_info:\n",
    "        # Reinitialize the buffer to clear any previous content in order to log the final dataframe info\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        logger.info(f\"Final DataFrame info:\\n \" + buffer.getvalue())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c86fb",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0cd3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file: str=\"../Data/DataSplits/test.csv\", label_column: str=\"Label\"):\n",
    "        \"\"\"Initializer for the Dataset class.\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file containing the dataset.\n",
    "            label_column (str): The name of the column indicating the label.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)   # Assign a pandas data frame\n",
    "        except FileNotFoundError:   # Raise an error if the file is not found\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Define feature and label columns\n",
    "        self.label_column = label_column\n",
    "        # Remove the Date column and the label column\n",
    "        self.feature_columns = self.data.columns.drop([self.label_column])\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"Returns a tuple (features, label) for the given index.\n",
    "        Args:\n",
    "            index (int): Index of the data sample to retrieve.\n",
    "        Returns:\n",
    "            tuple: (features, label) where features is a tensor of input features and label is the corresponding label.\n",
    "        \"\"\"\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        features = torch.tensor(row.drop(\"is_fraud\").values)    # Extract the features \n",
    "        label = torch.tensor(row[\"is_fraud\"])   # Extract the label for this item\n",
    "        return (features, label)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the amount of samples in the dataset.\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c267da",
   "metadata": {},
   "source": [
    "### Data Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df05a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(logger: Logger, dataset_url: str, root_data_dir: str= \"../Data\", data_file_path: str=\"Dataset.csv\", data_splits_dir: str=\"DataSplits\", scaler_dir = \"Scalers\", target_column: str=\"Target\", dropped_columns: Optional[List[str]]=None, batch_size: int=64, num_workers: int=0, pin_memory: bool=False, drop_last: bool=True) -> tuple[Dataset, Dataset, Dataset, DataLoader, DataLoader, DataLoader, MinMaxScaler, MinMaxScaler]:\n",
    "    \"\"\"This function prepares the train, test, and validation datasets.\n",
    "    Args:\n",
    "        logger (Logger): The logger instance to log messages.\n",
    "        dataset_url (str): The URL to download the dataset from, if not found locally.\n",
    "        root_data_dir (str): The root of the Data Directory\n",
    "        data_file_path (str): The name of the original dataset (with .csv file extension).\n",
    "        data_splits_dir (str): Path to the train, test, and validation datasets.\n",
    "        scaler_dir (str): Path to the feature and label scalers.\n",
    "        target_column (str): The name of the target column to predict.\n",
    "        dropped_columns (List[str], optional): Columns to drop from the features in original dataset.\n",
    "        batch_size (int): The dataloader's batch_size.\n",
    "        num_workers (int): The dataloader's number of workers.\n",
    "        pin_memory (bool): The dataloader's pin memory option.\n",
    "        drop_last (bool): The dataloader's drop_last option.\n",
    "\n",
    "    Returns: \n",
    "        train_dataset (Dataset): Dataset Class for the training dataset.\n",
    "        test_dataset (Dataset): Dataset Class for the test dataset.\n",
    "        validation_dataset (Dataset): Dataset Class for the validation dataset.\n",
    "        train_dataloader (DataLoader): The train dataloader.\n",
    "        test_dataloader (DataLoader): The test dataloader.\n",
    "        validation_dataloader (DataLoader): The validation dataloader.\n",
    "        feature_scaler (MinMaxScaler): The scaler used to scale the features of the model input.\n",
    "        label_scaler (MinMaxScaler): The scaler used to scale the labels of the model input.\n",
    "        \"\"\"\n",
    "    if not root_data_dir or not data_file_path or not data_splits_dir:  # Check for empty strings at the beginning\n",
    "        raise ValueError(\"File and directory paths cannot be empty strings.\")\n",
    "    DATA_ROOT = Path(root_data_dir)\n",
    "\n",
    "    DATA_CLEAN_PATH = DATA_ROOT / data_file_path # Set the path to the complete dataset\n",
    "\n",
    "    if DATA_CLEAN_PATH.exists():\n",
    "        logger.info(f\"CSV file detected, reading from '{DATA_ROOT}'\")\n",
    "        df = pd.read_csv(DATA_CLEAN_PATH, dtype='float32') # Convert data to float32 instead of, float64\n",
    "    else:\n",
    "        logger.info(f\"Downloading CSV file from Internet and saving into '{DATA_ROOT}'\")\n",
    "        try:\n",
    "            os.makedirs(DATA_ROOT, exist_ok=True)       # Create the Data Root Directory\n",
    "            df = pd.read_csv(dataset_url)  # Download and read the data into a pandas dataframe\n",
    "\n",
    "            # Clean the data before saving\n",
    "            try:\n",
    "                df = clean_data(df, logger, dropped_columns=dropped_columns)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"An unexpected error occurred cleaning the dataset\")\n",
    "\n",
    "            df.to_csv(DATA_CLEAN_PATH, index=False)     # Save the file, omitting saving the row index\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when downloading or saving the dataset from '{dataset_url}' to '{DATA_CLEAN_PATH}'\")\n",
    "\n",
    "    # Define the paths for the data splits and scalers\n",
    "    DATA_SPLITS_DIR = DATA_ROOT / data_splits_dir\n",
    "    SCALER_DIR = DATA_ROOT / scaler_dir\n",
    "\n",
    "    TRAIN_DATA_PATH = DATA_SPLITS_DIR / \"train.csv\"\n",
    "    TEST_DATA_PATH = DATA_SPLITS_DIR / \"test.csv\"\n",
    "    VALIDATION_DATA_PATH = DATA_SPLITS_DIR / \"val.csv\"\n",
    "\n",
    "    FEATURE_SCALER_PATH = SCALER_DIR / \"feature-scaler.joblib\"\n",
    "    # LABEL_SCALER_PATH = SCALER_DIR / \"label-scaler.joblib\"; Not used for this Classification task\n",
    "\n",
    "    if os.path.exists(TRAIN_DATA_PATH) and os.path.exists(TEST_DATA_PATH) and os.path.exists(VALIDATION_DATA_PATH) :\n",
    "        logger.info(f\"Train, Test, and Validation CSV datasets detected in '{DATA_SPLITS_DIR}.' Skipping generation and loading scaler(s)\")\n",
    "        try:\n",
    "            feature_scaler = joblib.load(FEATURE_SCALER_PATH)\n",
    "            # label_scaler = joblib.load(LABEL_SCALER_PATH)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when loading scalers: {e}\")\n",
    "    else:\n",
    "        logger.info(f\"Datasets not found in '{DATA_SPLITS_DIR}' or incomplete. Generating datasets...\")\n",
    "        os.makedirs(DATA_SPLITS_DIR, exist_ok=True)     # Create the Data Splits Parent Directory\n",
    "        os.makedirs(SCALER_DIR, exist_ok=True)     # Create the Scaler Parent Directory\n",
    "\n",
    "        # Create the scaler objects\n",
    "        feature_scaler = MinMaxScaler() \n",
    "        # label_scaler = MinMaxScaler(); Not for Classification\n",
    "        try:\n",
    "            df_features = df.drop(columns=[target_column], inplace=False)\n",
    "            df_labels = df[[target_column]]     # Instead of returning a pandas Series using \"[]\", return a dataframe using the \"[[]]\" to get a shape with (-1,1)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error Reading from DataFrame: {e}\")\n",
    "\n",
    "        # Use OverSampling Technique to Balance out the Dataset\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        df_features_resampled, df_labels_resampled = ros.fit_resample(df_features, df_labels)\n",
    "\n",
    "\n",
    "        # Split into smaller DataFrames for the Train, Test, and Validation splits\n",
    "        X_train, X_inter, Y_train, Y_inter = train_test_split(df_features_resampled, df_labels_resampled, test_size=0.1, random_state=42)\n",
    "        X_validation, X_test, Y_validation, Y_test = train_test_split(X_inter, Y_inter, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Now Fit the data\n",
    "        feature_scaler.fit(X_train)\n",
    "        # label_scaler.fit(Y_train); Not for Classification\n",
    "\n",
    "        # Save the fitted scaler object\n",
    "        try:\n",
    "            joblib.dump(feature_scaler, FEATURE_SCALER_PATH)\n",
    "            logger.info(f\"Feature scaler stored in: ({FEATURE_SCALER_PATH})\")\n",
    "            # joblib.dump(label_scaler, LABEL_SCALER_PATH)\n",
    "            # logger.info(f\"Label scaler stored in: ({LABEL_SCALER_PATH})\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when saving  Scalers: {e}\")\n",
    "\n",
    "        # Scale the rest of the data; returns numpy arrays\n",
    "        X_train_scaled = feature_scaler.transform(X_train)\n",
    "        # Y_train_scaled = label_scaler.transform(Y_train)\n",
    "        X_validation_scaled = feature_scaler.transform(X_validation)\n",
    "        # Y_validation_scaled = label_scaler.transform(Y_validation)\n",
    "        X_test_scaled = feature_scaler.transform(X_test)\n",
    "        # Y_test_scaled = label_scaler.transform(Y_test)\n",
    "\n",
    "        logger.info(f\"Train Features Scaled Shape: {X_train_scaled.shape}\")\n",
    "        logger.info(f\"Train Labels Shape: {Y_train.shape}\")\n",
    "        logger.info(f\"Validation Features Scaled Shape: {X_validation_scaled.shape}\")\n",
    "        logger.info(f\"Validation Labels Shape: {Y_validation.shape}\")\n",
    "        logger.info(f\"Test Features Scaled Shape: {X_test_scaled.shape}\")\n",
    "        logger.info(f\"Test Labels Shape: {Y_test.shape}\")\n",
    "        # Define the column names of the features and label\n",
    "        features_names = df_features.columns\n",
    "        label_name = df_labels.columns\n",
    "        # Create dataframes using the scaled data\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=features_names)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=features_names)\n",
    "        X_validation_df = pd.DataFrame(X_validation_scaled, columns=features_names)\n",
    "        Y_train_df = pd.DataFrame(Y_train, columns=label_name)\n",
    "        Y_test_df = pd.DataFrame(Y_test, columns=label_name)\n",
    "        Y_validation_df = pd.DataFrame(Y_validation, columns=label_name)\n",
    "\n",
    "        # Concatenate the features and labels back into a single DataFrame for each set\n",
    "        train_data_frame = pd.concat([X_train_df, Y_train_df.reset_index(drop=True)], axis=1)\n",
    "        test_data_frame = pd.concat([X_test_df, Y_test_df.reset_index(drop=True)], axis=1)\n",
    "        validation_data_frame = pd.concat([X_validation_df, Y_validation_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Saving the split data to csv files\n",
    "        try:\n",
    "            train_data_frame.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "            test_data_frame.to_csv(TEST_DATA_PATH, index=False)\n",
    "            validation_data_frame.to_csv(VALIDATION_DATA_PATH, index=False)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when saving datasets to CSV files: {e}\")\n",
    "    # Creating Datasets from the stored datasets\n",
    "    logger.info(f\"INITIALIZING DATASETS\")\n",
    "    train_dataset = CustomDataset(csv_file=TRAIN_DATA_PATH, label_column=target_column)\n",
    "    test_dataset = CustomDataset(csv_file=TEST_DATA_PATH, label_column=target_column)\n",
    "    val_dataset = CustomDataset(csv_file=VALIDATION_DATA_PATH, label_column=target_column)\n",
    "    \n",
    "    logger.info(f\"Creating DataLoaders with 'batch_size'=({batch_size}), 'num_workers'=({num_workers}), 'pin_memory'=({pin_memory}). Training dataset 'drop_last'=({drop_last})\")\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last, shuffle=True)\n",
    "    validation_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last, shuffle=False)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last, shuffle=False)\n",
    "\n",
    "    logger.info(f\"Training DataLoader has ({len(train_dataloader)}) batches, Test DataLoader has ({len(test_dataloader)}) batches, Validation DataLoader has ({len(validation_dataloader)}) batches\")\n",
    "\n",
    "    # return (train_dataset, test_dataset, val_dataset, train_dataloader, test_dataloader, validation_dataloader, feature_scaler, label_scaler)\n",
    "    return (train_dataset, test_dataset, val_dataset, train_dataloader, test_dataloader, validation_dataloader, feature_scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6b97a",
   "metadata": {},
   "source": [
    "# Test Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f1a6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_pipeline():\n",
    "    # Function input setup\n",
    "    data = {\n",
    "        \"dataset_url\":  \"hf://datasets/dazzle-nu/CIS435-CreditCardFraudDetection/fraudTrain.csv\",\n",
    "        \"root_data_dir\": \"../Data\",\n",
    "        \"data_file_path\": DATA_CLEAN_FILE_NAME,\n",
    "        \"data_splits_dir\": \"DataSplits\",\n",
    "        \"scaler_dir\": \"Scalers\",\n",
    "        \"target_column\": \"is_fraud\",\n",
    "        \"dropped_columns\": [\"Unnamed: 0\", \"trans_date_trans_time\", \"cc_num\", \"merchant\", \"first\", \"last\", \"street\", \"city\", \"zip\", \"job\", \"dob\", \"trans_num\", \"unix_time\", \"Unnamed: 23\", \"6006\"]\n",
    "    }\n",
    "    batch_size = 64\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    drop_last = True\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Call the data pipeline function\n",
    "    try:\n",
    "        (train_dataset, test_dataset, val_dataset, train_dataloader, test_dataloader, validation_dataloader, feature_scaler) = data_pipeline(\n",
    "        logger,\n",
    "        **data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Caught Exception: {e}\", stack_info=True)\n",
    "\n",
    "    # Basic assertions to verify the outputs\n",
    "    assert isinstance(train_dataset, Dataset), \"train_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(test_dataset, Dataset), \"test_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(val_dataset, Dataset), \"val_dataset is not an instance of Dataset\"\n",
    "    assert isinstance(train_dataloader, DataLoader), \"train_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(test_dataloader, DataLoader), \"test_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(validation_dataloader, DataLoader), \"validation_dataloader is not an instance of DataLoader\"\n",
    "    assert isinstance(feature_scaler, MinMaxScaler), \"feature_scaler is not an instance of MinMaxScaler\"\n",
    "    # assert isinstance(label_scaler, MinMaxScaler), \"label_scaler is not an instance of MinMaxScaler\"\n",
    "\n",
    "    logger.info(\"All assertions passed. Data pipeline test successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a32677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CSV file detected, reading from '..\\Data'\n",
      "INFO:__main__:Train, Test, and Validation CSV datasets detected in '..\\Data\\DataSplits.' Skipping generation and loading scaler(s)\n",
      "INFO:__main__:INITIALIZING DATASETS\n",
      "INFO:__main__:Creating DataLoaders with 'batch_size'=(64), 'num_workers'=(0), 'pin_memory'=(False). Training dataset 'drop_last'=(True)\n",
      "INFO:__main__:Training DataLoader has (29322) batches, Test DataLoader has (1629) batches, Validation DataLoader has (1629) batches\n",
      "INFO:__main__:All assertions passed. Data pipeline test successful.\n"
     ]
    }
   ],
   "source": [
    "test_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722951a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
